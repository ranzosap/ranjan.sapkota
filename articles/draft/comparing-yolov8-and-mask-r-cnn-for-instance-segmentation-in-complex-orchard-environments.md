---
    title: "Comparing YOLOv8 and Mask R-CNN for instance segmentation in complex orchard environments"
    authors: ["Ranjan Sapkota", "Dawood Ahmed", "Manoj karkee"]
    year: 2025
    month: 11
    publication_venue: "Science Direct"
    abstract: "Instance segmentation, an important image processing operation for automation in agriculture, is used to precisely delineate individual objects of interest within images, which provides foundational information for various automated or robotic tasks such as selective harvesting and precision pruning. This study compares the one-stage YOLOv8 and the two-stage Mask R-CNN machine learning models for instance segmentation under varying orchard conditions across two datasets. Dataset 1, collected in dormant season, includes images of dormant apple trees, which were used to train multi-object segmentation models delineating tree branches and trunks. Dataset 2, collected in the early growing season, includes images of apple tree canopies with green foliage and immature (green) apples (also called fruitlet), which were used to train single-object segmentation models delineating only immature green apples. The results showed that YOLOv8 performed better than Mask R-CNN, achieving good precision and near-perfect recall across both datasets at a confidence threshold of 0.5. Specifically, for Dataset 1, YOLOv8 achieved a precision of 0.90 and a recall of 0.95 for all classes. In comparison, Mask R-CNN demonstrated a precision of 0.81 and a recall of 0.81 for the same dataset. With Dataset 2, YOLOv8 achieved a precision of 0.93 and a recall of 0.97. Mask R-CNN, in this single-class scenario, achieved a precision of 0.85 and a recall of 0.88. Additionally, the inference times for YOLOv8 were 10.9 ms for multi-class segmentation (Dataset 1) and 7.8 ms for single-class segmentation (Dataset 2), compared to 15.6 ms and 12.8 ms achieved by Mask R-CNN's, respectively. These findings show YOLOv8's superior accuracy and efficiency in machine learning applications compared to two-stage models, specifically Mask-R-CNN, which suggests its suitability in developing smart and automated orchard operations, particularly when real-time applications are necessary in such cases as robotic harvesting and robotic immature green fruit thinning."
    tags: []
    published: false
    featured: false
    doi: "https://www.sciencedirect.com/science/article/pii/S258972172400028X"
    pdf_url: "https://pdf.sciencedirectassets.com/319956/1-s2.0-S2589721724X00033/1-s2.0-S258972172400028X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEO3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCICDHUoxb5rrvi%2FN%2BfW5ONq%2B3tb0tm7EHQXrZQgDlX6DhAiEAsbz2GFjHZe37FaKmzhBhk7wp%2FhpkPWbdk5MqzmN84N0quwUItv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDDVFGsqF%2FSGXOt1MgyqPBYwdTDHcvjmxMOTOCNtQxiqFoPicF9uBr1bXjFRd6sfVyBdEcwq30Yy2T7qlvkDOaBIomAXTAPnLen2T06Qjz1tmZu83nb3aVdCYMX6lTEJWn7vJypSdp3ZlM509EUTqZKc34T73b8ChTxM4SWVfeinQJzIAc7Xe37y3z1SJ5kSe0BhlLjN0pJLl1SQnDWAwVlJOWFhvAe7CBwh27olJk3rSVqGAmbSBIntNUHffkjV4nVcEUCArGKZ9tHfUSUn2B%2FhtQc7pADv%2Fg%2FcOM4QRrXuVi8tlHIyqcGJUi2tIO5ds5Q0g6qYqCFn0CNwTE2A8fGPsdnS53sAI281I97mwpiQwUn3CO2eoef5Efje8N90cNagt1%2Bl4VUty5V2VyAw1usGUQDiUos8CQ8lbD5QIhzBYxEKySgWqEEXyJ%2Bm1m%2FhaRbLZOHqKp1rPdExdVC02mCj37sEWDKJHkXh3uKNSDOMyjSrvxZlpwwnbFLUehB8Z5PZlegyyZ93WvckY8QDHYnqgX7%2Be3Ac7zmcCzEslKkVzpGib3Knl5bevD6c2woqq%2FjSPyC46gh8PV3QybLw3Fc9scDlhrdjzzyE5PHCBb8wFfZ%2B3vmIeA0S9GZVQlTtXtiANCaMuY8G3GtC1OpU8Ku45cBm4ywkkHaDen2uAEPsA2H2av4X4y65EqK0G82aiyULcHCgH8Mg3UmtNQuhEsofGdL%2Bi1ea0fxLnzLcszEGaq8lscDLTSbWLPEE7AgdJWx10FR%2BlxtNUDyuA2unel5%2FVbGpeJhfCZBtgDsJHW9FZiGVEU5rOm9VTT%2F%2FYOIlLBJBfxt7KxdimEtC8ChgPTFJxbOlbTr0uqyDoDZvEmA1NzLAK6lR6yn7tfU1MWOUw466myQY6sQGMofOTy%2BypdKV%2BFt1smlflhN0oJ6MPG8OLqwLgNFbOkONZDahuXu6UweNbv84bwYT1Y1%2BoVLdZnSX68JRrRf9wpnHRgtAl%2BnJ2mh46ue1OvEBcoUEC5pRRSPOrhk2PJDVPW0RZkhQe0W2uK%2FJI%2Beew5fBe8NFtgz8COTxoUrmTlDnY%2Br%2BT4%2FqBvAScKPAICCYuJHsod%2Fj7khz6BANj1su6QvVThBhGMIL43O1eJSzrzbU%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20251128T135618Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYTF3S4DGY%2F20251128%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=278b853fdb3cc5b70e9fbe37738c1d1b850681315aa7a1362819d98d17bfbb0e&hash=a834da1a0859f6b918d61d40f59db1fe1733418ba246984e91a757df42eb61f7&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S258972172400028X&tid=spdf-add14920-1b92-49cd-a897-d4b3cf65e921&sid=7211fb0e1618a34a902bd82481f65b198d7agxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1d045f05570255035600&rr=9a5a5e2efc6ccd57&cc=gb"
    code_url: "https://www.sciencedirect.com/science/article/pii/S258972172400028X"
    dataset_url: ""
    github_link: "https://github.com/ranzosap/ranjan.sapkota/blob/main/articles/published/comparing-yolov8-and-mask-r-cnn-for-instance-segmentation-in-complex-orchard-environments.md"
    ---

    1. Introduction
Instance segmentation is a powerful computer vision technique that combines the benefits of both object detection and semantic segmentation (Hafiz and Bhat, 2020). One of the key benefits of instance segmentation in agricultural applications is its ability to accurately quantify plant and crop structures (Zhang et al., 2020a, Zhang et al., 2020b), which can provide valuable information about plant growth, disease identification, and yield estimation, and can provide a foundation for various key areas of research and development such as robotic green (immature) fruit thinning (Champ et al., 2020). Instance segmentation can provide precise measurements of plant features, such as leaf area, stem length, and plant height, with a high level of accuracy and efficiency (Chen et al., 2019; Lüling et al., 2021). The traditional methods of instance segmentation in agricultural images were mostly based on hand-crafted features and classical image processing techniques such as Watershed Transform (Niu et al., 2016), Graph-based Segmentation (Pham and Lee, 2015), Active Contours (or Snakes) (Clement et al., 2013), level set (Gao et al., 2011; Jothiaruna et al., 2019; Ma et al., 2017), Region Growing (Gao et al., 2011; Jothiaruna et al., 2019; Ma et al., 2017), Morphological Operation (Gupta et al., 2017; Khirade et al., 2015) and Clustering-based methods (Arsan and Hameez, 2019; Tian et al., 2019a, Tian et al., 2019b). However, these methods require a lot of manual setups and refinements, making them time-consuming and less reliable (Ngugi et al., 2021). Additionally, these methods could not easily learn from new data, making them less flexible and difficult to adapt to different scenarios. Moreover, these methods involved multiple disjointed image processing stages such as noise removal, contrast adjustment, image enhancement, refinement and manually defining and/or extracting specific features such as edge, texture, or colors.
Transitioning from traditional image processing methods to deep learning-based techniques in instance segmentation represents a significant evolution in the analysis of agricultural imagery. Traditional methods such as Watershed Transform (Zeng et al., 2009), Graph-based Segmentation, and Active Contours rely heavily on predefined algorithms that segment images based on intensity gradients, colour, texture or connectivity, which often necessitates extensive manual tuning to extract the specific characteristics of different crops or conditions (Jayanthi and Shashikumar, 2019; Zeng et al., 2009). These techniques, while foundational in the early days of computer vision, require iterative adjustments and are limited by their inability to dynamically learn from new data or adapt to varied environments. In contrast, deep learning models, especially convolutional neural networks (CNNs), introduce layers of learning filters that automatically extract and learn the most informative features from vast amounts of data. Unlike traditional methods that depend on manual feature specification and are thus prone to human bias and error, deep learning systems learn to identify underlying patterns and irregularities in data/images, making them more robust and accurate. This capability is particularly beneficial in agricultural applications (e.g., canopy instance segmentation) where the variability in plant appearance and environmental conditions can be high (Coulibaly et al., 2022). CNN-based models offer end-to-end learning techniques, which not only reduces the processing time but also enhances the adaptability of the models to new, unseen scenarios, an essential feature for generalizable and scalable agricultural applications. This dynamic learning ability is a significant improvement over traditional methods that are static and constrained by their algorithmic rigidity.
More specifically, DL network architectures, including U-Net (Siddique et al., 2021), Mask R-CNN, and YOLO (Redmon et al., 2016), are increasingly utilized for a range of applications in agriculture. A key advantage of these DL techniques is their end-to-end learning approach, which enables direct mapping of raw images to segmentation results, thus enhancing consistency and reliability. Furthermore, transfer learning techniques allow for the adaptation of models pre-trained on extensive datasets to specific agricultural tasks, reducing both training times and data requirements. Utilizing these features of DL models, various agricultural applications have been investigated including plant disease identification (Rashid et al., 2023), yield prediction (Maji et al., 2022), pest detection (Lippi et al., 2021; Liu and Wang, 2020), soil health assessment (Qu et al., 2023), crop maturity analysis, and site-specific weed control application (Hu et al., 2021; Li et al., 2022; Wang et al., 2022) showcasing their versatility and efficiency in modern agricultural practices.
As mentioned before, instance segmentation techniques have been widely applied to crop disease management (Chen et al., 2021a, Chen et al., 2021b). Early detection of plant diseases is crucial for maintaining crop yield and quality. Utilizing instance segmentation, researchers can quantify symptoms such as leaf spots and discoloration and monitor the progression of these diseases over time (Tian et al., 2020). This capability is instrumental in developing effective disease management strategies, including targeted treatments and breeding for disease-resistant cultivars. Instance segmentation has also been proved pivotal for precise crop yield estimation. Accurate yield estimation is essential for growers and breeders to make informed decisions about crop management and to select traits for breeding new cultivars. Instance segmentation techniques can be used to accurately counting and sizing individual fruits or other canopy objects from images. Such information facilitates a precise yield estimation and provides key insights into cultivar characteristics (Chen et al., 2021a, Chen et al., 2021b). Past studies have demonstrated the effectiveness of these techniques in various relevant applications such as the segmentation of apple flowers (Tian et al., 2020), segmentation and localization of strawberry fruits for harvesting segmentation and counting of cranberries, and the segmentation of guava fruits and branches (Lin et al., 2021). The data derived from these studies assist in optimizing crop management strategies, including optimal application of water and fertilizers, and identifying high-yielding cultivars (Tian et al., 2020) as mentioned before.
In addition, instance segmentation has been applied extensively to develop machine vision systems for agricultural robots because it provides capabilities for robots to detect, delineate, and track individual objects of interest in agricultural fields using images or videos, such as fruits, branches, flowers, vegetables, and livestock (Jha et al., 2019). Detecting and tracking plant parameters such as leaves, stem, trunk, branch, flower, and fruit is necessary for a robot to automatically perform various tasks such as harvesting, and canopy, and crop-load management operations. In the last few years, several studies have implemented the use of deep learning-based instance segmentation techniques for developing robotic solutions for various agricultural applications such as tree pruning in dormant season picking fruits and vegetables (Jia et al., 2020; Yu et al., 2019; Zu et al., 2021), thinning flowers and fruitlet and identifying and killing weeds (Xie et al., 2021) among others.
Among the broad applications of deep learning techniques in agriculture, there has been a focus on the use of two specific architectures: YOLO (You Only Look Once) and Mask Region-Based Convolutional Neural Network (Mask R-CNN). These models, known for their effectiveness in instance segmentation, have been pivotal in advancing tasks such as crop detection, pest and disease management, weed identification, tree canopy segmentation, and canopy object (e.g., branch and fruit) detection. These tasks, critical in precision and automated agriculture, benefit immensely from the capabilities of these two deep learning models. As mentioned before, many recent studies conducted in agricultural applications used Mask-R-CNN-based (He et al., 2017) instance segmentation for tasks such as crop detection (Ganesh et al., 2019; Wang et al., 2021), pest and disease detection (Afzaal et al., 2021; Lin et al., 2020; Rehman et al., 2021), weed detection (Osorio et al., 2020), tree canopy segmentation (Safonova et al., 2021; Zhao et al., 2018), and tree branch detection (Safonova et al., 2021; Zhao et al., 2018). Concurrently, the YOLO family of models has been used widely in object detection because of its ability to handle tasks like object detection, image classification, and instance segmentation simultaneously with one-stage networks. Unlike Mask R-CNN, a two-stage model suitable for segmentation tasks (Soviany and Ionescu, 2018), YOLO optimizes the overall processing ensuring speed and efficiency crucial for real-time applications in agriculture such as robotic pruning, thinning (Hussain et al., 2023), and pesticide application (Seol et al., 2022).
A number of recent studies focused on the segmentation of tree trunks and branches, employing various deep learning approaches. For example, (Fu et al., 2023; Ma et al., 2021) used deep learning for automatic branch detection in jujube trees and (Zhang et al., 2018) used Regions-Convolutional Neural Networks (R-CNN) models alongside depth features for branch detection in fruiting wall apple trees. Segmenting plant canopy parts in dormant grapevines has also been studied widely using different deep learning techniques (e.g., (Guadagna et al., 2023)). Other models like ViNet (Gentilhomme et al., 2023) have emerged, providing deep learning solutions for estimating grapevine structures. Further advancements include the application of deep learning and geometric constraints for obscured branch segmentation and three-dimensional reconstruction (Kok et al., 2023), as well as the use of space colonization algorithms for dormant pruning in jujubee plants (Fu et al., 2023). A deep learning-based sensing system (called SPGnet) for jujubee plant by Baojian et al. (Ma et al., 2021), branch detection in apple trees using R-CNN by Zhang et al., 2018, and Lin et al.'s bab (Lin et al., 2021) tiny Mask R-CNN for guava branch reconstruction are other recent studies in this field. Additionally, Aguiar et al. (Aguiar et al., 2021) explored trunk segmentation using a semantic segmentation-based deep learning approach with a Single Shot Multibox Detector (SSD). In comparison with the performance measures reported by these latest, innovative methodologies available in the literature, YOLOv8 model presented in this study performed better in segmenting tree trunks in terms of both precision (0.95), recall (0.97) and mAP@0.5(0.74). Furthermore, while the Mask R-CNN model achieved relatively lower performance relative to YOLOv8, its performance was comparable or better with many recent studies on trunk and branch detection including (Gentilhomme et al., 2023; Guadagna et al., 2023; Kok et al., 2023; Xiang et al., 2022; Zhang et al., 2018).
Both models are extensively studied, as discussed above and as shown in Table 1, highlighting 23 publications in the last 3 years focusing specifically on analyzing images of modern apple tree canopies.
Table 1. Highlighting the studies conducted in the last three years on YOLO and Mask R-CNN during different apple orchard environments.

References	Year	DL model	Objectives
(Chen et al., 2021a, Chen et al., 2021b; Wu et al., 2021)	2021	YOLO-V4	Apple detection in a complex scene
(Huang et al., 2021; Liu et al., 2021)	2021	Mask R-CNN	Deep learning-based apple detection
(Huang et al., 2021; Kuznetsova et al., 2021)	2021	YOLO-V3	Green fruit detection (apples, mangoes)
(Kuznetsova et al., 2021; Wang et al., 2021)	2021	YOLO-V5	Apple fruitlet detection for fruitlet thinning
(Tong et al., 2022)	2022	Mask R-CNN	Branch identification and junction points localization in apple trees; Trunk identification and segmentation
(Gao et al., 2022; Zhang et al., 2022)	2022	YOLO-V4	Apple detection, counting, and tree trunk tracking in modern orchards
(Lu et al., 2022)	2022	YOLO-V4	Immature/mature apple detection on dense-foliage tree architectures for early crop-load estimation
(Lv et al., 2022)	2022	YOLO-V5	Identification method for the apple growth pattern in the orchard
(Su et al., 2022)	2022	YOLO-V5	Tree trunk and obstacle detection in apple orchards
(Jia et al., 2022)	2022	Mask R-CNN	Ripe and green apple segmentation in orchards
(Cong et al., 2022)	2022	Mask R-CNN	Tree and tree crown segmentation in orchards
(Karthikeyan et al., 2023)	2023	YOLO-V3	Apple fruit quality detection
(Ma et al., 2023)	2023	YOLO-V7	Detection and counting of small target apples
(Hussain et al., 2023; Jia et al., 2022)	2023	Mask R-CNN	Green apple segmentation
Building upon this background of widespread application of YOLOv8 and Mask R-CNN models, the primary goal of this study is to systematically compare and evaluate the performance of these two models (YOLOv8 and Mask R-CNN) for instance segmentation tasks in modern, commercial apple orchards. Through this comprehensive comparison, this research aims to provide insights into the suitability, efficiency, and potential challenges associated with implementing each model in agricultural automation applications. To achieve this goal, the following specific objectives will be pursued in this study:
1.
To compare the performances of YOLOv8 and Mask R-CNN models in segmenting single-class objects, specifically green apples (fruitlets), in images collected from variable orchard environments in the early growing season; and
2.
To evaluate the capabilities of these two models in segmenting multi-class objects, specifically primary branches and tree trunks of apple trees in images collected from a model apple orchard during the dormant season.
The comparison between YOLOv8 and Mask R-CNN in this study is founded on the significant advancements in the YOLO architecture that extend its capabilities beyond mere bounding box detection. Traditionally, while YOLO models were primarily known for their speed and efficiency in object detection, the latest iterations, particularly YOLOv8, have incorporated features supporting instance segmentation. This adaptation allows YOLOv8 to not only predict bounding boxes but also to generate precise object masks, aligning its functionalities more closely with those of Mask R-CNN, which has been a standard in instance segmentation. Therefore, comparing these two models is pertinent as both now offer robust solutions for instance segmentation, making the evaluation of their performance in agricultural applications, where both detection speed and segmentation accuracy are critical, highly relevant and scientifically justified.
The remainder of this paper is organized to provide a comprehensive comparison of YOLOv8 and Mask R-CNN models, specifically focusing on their application in commercial apple orchards. First, a “Background” section is presented to outline the theoretical frameworks of the one-stage (YOLOv8) and two-stage (Mask R-CNN) detectors, setting the stage for a deeper understanding of these complex models. Then a “Materials and Methods” section is provided to describe the experimental design, data acquisition, and analytical methodologies employed. Following the methodology, a “Results and Discussion” section is presented to report the results and critically discuss models' performance in various segmentation tasks, including their efficiency and efficacy. The paper concludes with a “Conclusion” section, which summarizes the research methods and the findings of the study. The paper ends with “Future Work” section, which summarizes the potential further comparison with state-of-the-art other models.
2. Deep learning models
Deep learning models used for object detection are generally categorized into two distinct approaches: one-stage and two-stage detectors (Carranza-García et al., 2020). Two-stage detectors, such as Mask R-CNN, first generate regions of interest (ROIs) in an initial stage, using a Region Proposal Network (RPN) (He et al., 2017). These regions are then classified and refined in the second stage to provide precise object localization and classification. This approach is known for its high accuracy due to the focused refinement of detected objects. On the other hand, one-stage detectors like YOLO streamline this process by directly predicting object classes and bounding boxes in a single pass through the network, sacrificing some accuracy for significant gains in speed. One-stage models do not separate the detection into distinct region proposals and refinement stages, allowing them to operate faster, making them well-suited for applications requiring real-time processing (Carranza-García et al., 2020). Both methodologies have continued to evolve in recent years to address the trade-offs between speed and accuracy.
It is true that numerous one-stage and two-stage detectors have been developed over the past decade, each tailored for specific performance criteria in terms of speed and accuracy. Some of the most widely used two-stage detectors include Fast R-CNN (which improves the efficiency of feature usage), Faster R-CNN (which integrates a region proposal network), and R-FCN and Cascade R-CNN (which enhance localization and classification accuracy through specialized networks). Similarly, most widely used one-stage detectors include SSD (Single Shot MultiBox Detector) and RetinaNet (which introduced focal loss to handle class imbalance), and family of YOLO model including YOLOv5 and YOLOv6 (which emphasize detection speed). Despite the availability of numerous one-stage and two-stage models, YOLOv8 and Mask R-CNN have been the most widely used models in agricultural applications with highly impactful results (Duong-Trung and Duong-Trung, 2024; Jabir et al., 2023; Mu et al., 2023; Xu et al., 2020; Yang et al., 2023; Yu et al., 2022; Yue et al., 2023).Comparative studies have clearly demonstrated their superior performance in detecting and segmenting agricultural objects under varied conditions, as outlined in Table 1 of this manuscript. These models balance the trade-offs between speed, accuracy, and robustness, making them especially suitable for the dynamic environments encountered in agricultural settings. Based on these concaving past studies, Mask R-CNN (for its precise segmentation capability), and YOLOv8 (for its exceptional speed), have thus been selected for this study.
2.1. Mask R-CNN
Mask R-CNN is a deep learning model designed for object detection and instance segmentation, renowned for its accuracy and efficiency. Its strength lies in its ability to precisely identify and delineate each object in an image, making it highly effective for complex image analysis tasks. The model was developed by researchers at Facebook AI Research in 2017 and builds on top of the Faster R-CNN object detection model by adding a branch for predicting object masks in parallel with the existing branch for bounding box detection (He et al., 2017). The architecture of Mask R-CNN consists of three main components: a backbone network, a region proposal network (RPN), and two parallel branches for bounding box detection and mask prediction as shown in Fig. 1. The backbone network is typically a convolutional neural network (CNN) that extracts features from the input images and is shared by both branches. The RPN generates a set of region proposals that are likely to contain objects, based on the feature maps generated by the backbone network. The bounding box branch predicts the class label and bounding box coordinates for each region proposal, while the mask branch predicts a binary mask for each object instance within the bounding box.

![1-s2.0-S258972172400028X-gr1](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/1ae4b59a-7d9f-45f7-80a9-cb5ce89b4cb0-1-s2-0-S258972172400028X-gr1.jpg)
Fig. 1. Mask-R-CNN architecture with; (a) structure diagram, highlighting the backbone network, RPN, bounding box, and mask prediction branches; and (b) detailed view of the Region Proposal Network (RPN).

However, the application of Mask R-CNN and other deep learning models in agriculture comes with several challenges. First, the performance of the model heavily relies on the quality and diversity of the training dataset. Agricultural environments are highly variable, with changes in lighting, weather conditions, and plant growth stages, all of which can affect the model's accuracy (Hoogenboom, 2000). Moreover, Mask R-CNN requires substantial computational resources for training and inference (Zhang et al., 2020a, Zhang et al., 2020b), which can be a limitation in real-time applications on the farm where such resources are limited.
Ongoing studies are focusing on addressing these challenges and optimizing deep learning models such as Mask R-CNN for improved efficiency and robustness. These efforts include integrating more adaptive and scalable neural network architectures, improving data augmentation techniques to make the model more resilient to environmental variabilities, and developing lightweight versions of the models that maintain high accuracy while being more resource efficient. For instance, Mask R-CNN has been deployed to identify specific picking points on tea plants (Wang et al., 2023a, Wang et al., 2023b) to aid robotic harvesting of quality tea-leaves while minimizing damage to plants. Furthermore, this model has shown promising results in various horticultural applications such as assessing the ripeness of strawberries (Tang et al., 2023). By combining Mask R-CNN with region segmentation techniques, the system effectively distinguished between different ripeness stages, enabling growers to optimize the timing of strawberry picking for better market prices and reduced waste. In apple orchards, Mask R-CNN has been utilized for flower detection and identification of the king flower, which is critical for targeted pollination strategies (Amogi et al., 2023). This application is expected to help improve pollination efficiency leading to higher fruit yield and quality. Mask R-CNN has also been applied to monitor crop stress such as estimating fruit surface temperature using IoT sensors. This technology allows for real-time monitoring and management of fruit crops, thus helping orchard managers to mitigate the effects of heat stress and maintain fruit quality and yield.
2.2. YOLOv8
The YOLO family of object detection and instance segmentation models have evolved rapidly over the last several years, with each new iteration introducing improvements in accuracy and/or speed. YOLOv8 (Fig. 2), the latest one-stage model, was built on the foundations provided by previous YOLO models, such as YOLOv3 and YOLOv5. Compared to two-stage models, YOLOv8 directly predicts bounding boxes and class probabilities without the need for a separate region proposal network, streamlining the object detection process. One key innovation in YOLOv8 is the adoption of an anchor-free, center-based approach for object detection, which offers several advantages over traditional anchor-based methods such as YOLOv5, YOLOv6, and YOLOv7. YOLOv8 implements Pseudo Ensemble or Pseudo Supervision (PS), a method that involves training multiple models with distinct configurations on the same dataset to generate a more diverse set of predictions, improving the accuracy and robustness of the final prediction. Additionally, YOLOv8 leverages the Darknet-53 architecture, a 53-layer deep convolutional neural network optimized for feature extraction and object detection. One significant architectural change in YOLOv8 is the replacement of the C3 module with the C2F module. The C3 module, also known as the convolutional module, processes input data through a series of convolutional operations. The C2F module, an improved version of the C3 module, enhances accuracy and processing times compared to previous models. Furthermore, YOLOv8 substitutes the 6 × 6 Convolutional (Conv) layer with a 3 × 3 Conv layer in the model backbone, reducing the number of parameters and creating a more compact, computationally efficient network. YOLOv8 also employs a decoupled head, which separates the tasks of predicting object presence and classifying object types, thereby improving both accuracy and processing speed. This refinement positions YOLOv8 as an effective solution for both object detection and instance segmentation in computer vision.


![1-s2.0-S258972172400028X-gr2](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/a181f0fb-e172-41d5-9be7-f33019f0c399-1-s2-0-S258972172400028X-gr2.jpg)


Fig. 2. YOLOv8 architecture showcasing its innovative design for object detection and segmentation (https://deci.ai/blog/history-yolo-object-detection-models-from-yolov1-yolov8/).

YOLOv8 offers several configurations to cater to different needs for computational speed and accuracy (Li et al., 2023; Wang et al., 2023a, Wang et al., 2023b): YOLOv8-Tiny for fast processing at the cost of some accuracy, ideal for real-time applications on limited-resource devices; YOLOv8-Small, which balances speed with more detailed detection capabilities; YOLOv8-Standard for robust performance in diverse settings; and YOLOv8-Large, which prioritizes high accuracy for critical applications where details and precision are paramount.
The recent advancements in YOLOv8 have facilitated its adoption in diverse agricultural applications, demonstrating effectiveness in addressing specific challenges inherent to various farming environments discussed earlier. By optimizing the processing of low-level features, YOLOv8 becomes a powerful tool for the early detection of subtle signs of agricultural pests and diseases, critical for maintaining crop health (Zhang et al., 2023). For example, enhanced versions of YOLOv8 have been applied to detect diseases in vegetables within greenhouse environments (Wang and Liu, 2024) ensuring early detection and management of plant diseases. Further innovations include the integration of attention mechanisms into YOLOv8 to enhance the object detection capabilities, which was tested by (Yang et al., 2023) to improve tomato detection accuracy in cluttered agricultural environments.
3. Materials and methods
This study consisted of four major steps as outlined in Fig. 3a beginning with RGB images acquisition from commercial orchards in two distinct seasons (Fig. 3b as dormant season and 3c as early growing season). These images, captured under varying environmental conditions such as bright and cloudy days, were then manually annotated to create the training and testing datasets. The training dataset was subsequently used to train the two deep learning models mentioned previously, and their performance in instance segmentation was evaluated using the test dataset.


![1-s2.0-S258972172400028X-gr3](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/39601ee6-3c6d-4758-a7ae-14648fc7f7c5-1-s2-0-S258972172400028X-gr3.jpg)

Fig. 3. (a) Overall workflow diagram used in this research; (b) An example image of an Apple orchard during the dormant season (November 22, 2022); (c) An example image of an apple orchard during early fruit growing season (June 18, 2023); (d) IntelRealsense 435i camera used to acquire images to train and test the instance segmentation models; (e) Example trunks and branches used to annotate the dormant season images; and (f) Example immature green fruits (fruitlets) used to annotate early growing season images. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

3.1. Study site and data acquisition
This study was conducted in a commercial apple orchard (Fig. 3b and c) owned and operated by Allan Brothers Fruit Company, located at Prosser, Washington State, USA. The orchard was planted in 2009 with a Scilate apple cultivar with a row spacing of 9.0 ft., and a plant spacing of 3.0 f. and was trained to a V-trellis architecture. Two sets of RGB images were acquired using IntelRealsense 435i (Intel Corporation, California, USA); one in November 2022 creating dormant season dataset as shown in Fig. 3b and e, while the other set of images was acquired in June 2023 (just before manual fruitlet thinning) which provided the dataset for early growing season as illustrated in Fig. 3c and f. The Intel RealSense camera was selected for capturing RGB images due to its software development kit's (Intel RealSense SDK) ability to adjust parameters and capture high-quality images.
3.2. Data preparation
Two kinds of datasets comprising 1553 RGB images, capturing a variety in orchard lighting conditions were prepared for analysis of the deep learning model. Dataset 1 comprised 474 images from the dormant season, which were annotated manually to represent multi-class objects: the tree trunk and primary branches growing out from the trunks (Fig. 4). Altogether, 1141 annotations for the tree trunk and 2369 annotations for the tree branches were generated manually by creating the polygon over desired objects in these images using the image labeling software Labelbox. Likewise, dataset 2 comprised 1079 images from the green fruit growing season in which 5921 annotations of immature green apples were generated. During the image preprocessing stage using the label box software, all these annotations were formatted in accordance with the COCO dataset specification, which meets the requirement of both the YOLOv8 and Mask R-CNN model for image segmentation.


![1-s2.0-S258972172400028X-gr4](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/e31d7d56-b6cb-405d-9be4-82a401788fa5-1-s2-0-S258972172400028X-gr4.jpg)


Fig. 4. Workflow diagram showing the two types of datasets used in the study; Dataset 1 included the dormant season apple trees with multi-class objects (Trunk and branch) and Dataset 2 included growing season apple tree canopies with immature green fruits. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

Furthermore, to facilitate model training and validation, both datasets were resized to 640 × 640 pixels, and both datasets were divided randomly into training, validation, and test subsets, following an 8:1:1 distribution ratio for each object class.
3.3. Deep learning model implementation
Both the YOLOv8 and Mask R-CNN models were trained on a workstation with an Intel Xeon® W-2155 CPU @ 3.30 GHz x20 processor, NVIDIA TITAN Xp Collector's Edition/PCIe/SSE2 graphics card, 31.1 GiB memory, and Ubuntu 16.04 LTS 64-bit operating system. The backend framework for the model implementation was Pytorch, operating on a Linux system. To optimize performance, the learning rate used was 0.001, the batch size used was 32, and the dropout rate used was 0.5 to mitigate overfitting. The training was conducted over 1000 iterations. The model training was stopping before reaching 1000 epochs, if the model performance did not improve for 20 consecutive epochs over the validation dataset, which was useful to minimize model overfitting to the training dataset and improving generality. An initial learning rate of 0.01 was used in training both models, whereas the momentum and weight decay used were 0.937 and 0.0005 respectively for the two models. These parameter settings were chosen to optimize the speed of the training process while minimizing the chances of overfitting the model to the training dataset. During the initial three epochs, a warm-up phase was employed, using a momentum of 0.8 and a bias learning rate of 0.1, to stabilize the model's optimization and mitigate the risk of being stuck at a poor local minimum.
During the training process, various augmentation techniques were applied to enhance model robustness and generalization such as hue augmentation (0.015), saturation augmentation (0.7), value augmentation (0.4), translation adjustments (0.1), scaling variations (0.5), and a 50% probability for left-right flips. Additionally, a mosaic augmentation was applied with a probability of 1.0. After the model training was completed, the model outputs were converted to TorchScript format to simplify further processing to evaluate the performances of both YOLOv8 and Mask R-CNN models in terms of precision, recall, mean average precision (mAP), and area under curve (AUC) as discussed below. The detailed the specific data augmentation techniques and hyperparameter values utilized during training is presented in Table 2:
Table 2. Data augmentation and regularization parameters used in training models in this study.

Methods Applied	Value
Hue augmentation (fraction)	0.015
Saturation augmentation (fraction)	0.7
Value augmentation (fraction)	0.4
Rotation	0.0
Translation	0.1
Scale	0.5
Flip left-right (probability)	0.5
Mosaic (probability)	1.0
Weight decay	0.0005
The number of training epochs was determined through a preliminary test. In this test, the model performance with the validation dataset was monitored during the training process and the number of epochs when validation accuracy started to decrease or stay flat was chosen as the optimal number of epochs, which was expected to help avoid model overfitting.
3.4. Performance evaluation
To evaluate the instance segmentation capabilities of the Mask R-CNN and YOLOv8 models, five distinct metrics were used: Precision, Recall, mean Average Precision (AP) at 0.5 intersection over union (mAP@0.5 IOU), Area Under the receiver operating characteristic Curve (AUC), and Inference speed. Precision is defined as the proportion of correctly identified positive instances to the total predicted positive instances, as depicted by Eq. 1. Similarly, recall, depicted by Eq. 2, quantified the proportion of correctly identified positive instances out of all actual instances of the target objects. Furthermore, the mean average precision (mAP), represented as the average of the AP across k categories (Eq. 4), was crucial in evaluating the model's precision at a threshold of 50% overlap between predicted and true object boundaries/bounding boxes. The area under the curve (AUC), defined by Eq. 5, assessed the model's classification efficacy across all possible thresholds. The model's efficiency in processing and delivering predictions was measured by the inference speed and was inversely related to the time taken per image analysis.
These metrics are calculated as follows:


![Screenshot 2025-11-28 134929](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/0fb10ef2-66e5-436e-9795-5be900bb41b5-Screenshot-2025-11-28-134929.png)

where TP, FP, and FN represent true positive, false positive, and false negative object instances respectively. Variable ‘k’ represents the total number of object classes, and (AP)i refers to the average precision calculated for the ith class among these k classes. AP is the area under the precision-recall curve for a given class. TPR represents the true positive rate, FPR is the false positive rate, and t indicates the time taken for the model to infer results for a given (single) image.
4. Results and discussion
4.1. Single-class object segmentation of immature green apples (fruitlets)
For single-class segmentation for immature green fruits, the Precision-Confidence curves depicted in Fig. 5, revealed that the YOLOv8 model achieved a maximum precision of 1.00 when the confidence threshold was 0.929 (Fig. 5a). Correspondingly, Recall-Confidence curves for the respective models are presented in Fig. 6, which showed that YOLOv8's recall reached 0.97 at the minimum confidence threshold of 0.000. This high recall rate, or sensitivity, indicates the model's ability to correctly identify a high percentage of actual objects, which showed models effectiveness in segmenting green fruits even at the lowest confidence levels. Additionally, YOLOv8 outperformed Mask R-CNN in terms of mean average precision (mAP), achieving 0.939 at a 0.5 IoU threshold for green fruits and overall categories, compared to mAP of 0.902 achieved with Mask R-CNN (Fig. 7).


![1-s2.0-S258972172400028X-gr5](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/eac9120e-f5ab-4556-8245-a9a53128e2b2-1-s2-0-S258972172400028X-gr5.jpg)


Fig. 5. Precision-Confidence curve for single class segmentation of immature green apples (fruitlets) using; (a) YOLOv8; and (b) Mask R-CNN. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)


![1-s2.0-S258972172400028X-gr6](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/b7bb385b-79ec-4691-99d0-7f3489d8f666-1-s2-0-S258972172400028X-gr6.jpg)

Fig. 6. Recall-Confidence curve for single class segmentation of green apple fruits using; (a) YOLOv8; and (b) Mask R-CNN. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)


![1-s2.0-S258972172400028X-gr7](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/66c24420-a949-41d6-ba36-b378acab83f9-1-s2-0-S258972172400028X-gr7.jpg)


Fig. 7. Precision-Recall curve for single class segmentation of green apple fruits at mAP@0.5; (a) YOLOv8; and (b) Mask R-CNN. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

The performance differences between YOLOv8 and Mask R-CNN generally reflected the distinct nature of their architectures and the way they process images. YOLOv8, being a one-stage detector, is designed for speed and accuracy, making it capable of excluding similar non-target areas, as observed in the segmentation tasks (Fig. 8b). Its direct approach to object detection avoids the region proposal step, leading to fewer false positives in areas of the canopy that resembled the target fruit in colour. Mask R-CNN, on the other hand, uses a two-stage process, which involved generating region proposals before classifying and segmenting objects. This can sometimes result in the inclusion of non-target areas, such as leaves and stems being misclassified as fruits (Fig. 8c). Moreover, its performance appears to be more sensitive to lighting variations, which can lead to errors in object identification under the extreme sides of lighting situation such as bright, direct sun-light and dark shadows (Fig. 9c).


![1-s2.0-S258972172400028X-gr8](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/bd1bdd6e-7ad6-423c-9f86-9a9126ffde0e-1-s2-0-S258972172400028X-gr8.jpg)


Fig. 8. Example images showing the performance of two methods in segmenting immature green fruit in orchard condition; (a) Original images; (b) Instance segmentation results of YOLOv8; and (c) Instance segmentation results of Mask R-CNN. It is noted that some problematic regions in the canopy images (yellow circles) were incorrectly segmented as green fruit by Mask R-CNN but were correctly left as background by YOLOv8. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)


![1-s2.0-S258972172400028X-gr9](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/d9d53370-f8a4-4d26-bb34-c2821651e50e-1-s2-0-S258972172400028X-gr9.jpg)

Fig. 9. Figure illustrating wrong detection during the growing season orchard condition, yellow region includes the focus area (a) Original image 1; Instance segmentation results of YOLOv8; and (c) Instance segmentation results of Mask R-CNN. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

Despite these differences, there are specific situations where Mask R-CNN could still be the preferred choice. Its two-stage process, particularly the region proposal step, can be advantageous in complex segmentation tasks where precision is critical, and objects are densely packed or partially obscured. In the past, green fruit segmentation has been investigated using various approaches. Wei et al.'s D2D framework (Wei et al., 2022), GHFormer's focus on night-time detection (Sun et al., 2022), Liu et al.'s FCOS model for obscured fruits (Liu et al., 2022), Jia et al.'s ResNet-based FoveaMask (Jia et al., 2021), and Sun et al.'s combination of GrabCut and Ncut algorithms (Sun et al., 2019) each offered solutions to specific segmentation challenges such as lower accuracy and higher computation cost. Some studies also explored semi-automated models (Prabhu and Rani, 2021). However, the performance of the YOLOv8 model in this study exceeded those of the reviewed past studies. Likewise, the performance of the Mask R-CNN model in segmenting immature green fruits, while not as high as YOLOv8's, still surpassed many recent approaches (Karthikeyan et al., 2023; Liu et al., 2022; Lu et al., 2022; Prabhu and Rani, 2021; Sun et al., 2022; Tian et al., 2019a, Tian et al., 2019b).
Based on the recently published results such as multi-class fruit detection using a robotic vision system by Wan et al. (Wan and Sotirios, 2020), the authors compared YOLOv3, Faster-R-CNN and Improved Faster-R-CNN, and achieved a mAP% of 84.89%, 82.56% and 86.41% respectively. Based on the performance measures achieved in this study (e.g., 90.2% for YOLOv8 and 85% for Mask R-CNN) for single class datasets, it was observed that YOLOv8 and Mask R-CNN has a potential to achieve a substantially better performance compared to the same achieved with the other models. However, further study to compare the performance of all these models with the same dataset would be essential to further substantiate this finding.
4.2. Multi-class object segmentation in images of dormant apple trees
Similar to single class object segmentation discussed above, YOLOv8 performed better than Mask R-CNN in segmenting dormant apple tree images into multiple object classes (trunks and branches). YOLOv8 achieved a precision of 1.00 at a confidence threshold of 0.906, as shown in Fig. 10. Similarly, Fig. 11 shows that the recall for YOLOv8 reached 0.95 at the minimal confidence threshold, indicating a high degree of accuracy in segmenting these complex structures of dormant tree canopies. Mask R-CNN reached a precision of 1.00 at a lower confidence threshold of 0.813, suggesting a strong ability to correctly detecting target objects at this level of confidence (Fig. 10b). Additionally, the recall of Mask R-CNN, as depicted in Fig. 11, achieved 0.837 at the lowest confidence threshold, indicating slightly higher rate of false negatives compared to YOLOv8. Similarly, precision-recall curve (Fig. 12a) showed that YOLOv8 achieved a mean average precision (mAP) of 0.845 over all object classes at an intersection over union (IoU) of 0.5, which for the trunk and branch classes were 0.971 and 0.719, respectively. Mask R-CNN achieved relatively lower performance in multi-class segmentation tasks as well. As seen in Fig. 12b (precision-recall curve) the model achieved an all-class mAP of 0.748 at an IoU of 0.5, with individual mAP of 0.828 for trunk segmentation and 0.673 for branch segmentation.


![1-s2.0-S258972172400028X-gr10](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/901617a9-2012-4832-9b5c-6e710dd3d7e6-1-s2-0-S258972172400028X-gr10.jpg)


Fig. 10. Precision-Confidence curve for multi-class segmentation of Trunk and Branch; (a) YoloV8, (b) Mask R-CNN.


![1-s2.0-S258972172400028X-gr11](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/e8b61529-ec6d-42a4-b5ef-413174b3fc18-1-s2-0-S258972172400028X-gr11.jpg)

Fig. 11. Recall-confidence curve for multi-class segmentation of trunks and branches achieved with; (a) YOLOv8; and (b) Mask R-CNN.



![1-s2.0-S258972172400028X-gr12](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/4a619ef4-d939-49e2-852f-3c39a59cbd18-1-s2-0-S258972172400028X-gr12.jpg)

Fig. 12. Precision-Recall curve for multi-class segmentation of trunks and branches of dormant apple trees at mAP@0.5; (a) with YOLOv8; and (b) with Mask R-CNN.

Example images demonstrating comparative successes and failures of these models (YOLOv8 and Mask R-CNN) in segmenting trunks and branches are depicted in Fig. 13, Fig. 14. As shown before with mAP and other measures, trunks were segmented with higher accuracy by YOLOv8 compared to Mask R-CNN, which are indicated by sample cases shown in shown in Fig. 13b and c respectively. Specifically, the branch highlighted within the yellow dotted rectangle (Fig. 13 a, b and c) was successfully detected by YOLOv8 but not by Mask R-CNN, showing YOLOv8's better performance in low light conditions compared to Mask R-CNN. The example in Fig. 13 shows that YOLOv8 was more effective in segmenting trunks. Similarly, Fig. 14 presented examples of successful and failed segmentations in both trunk and branches, which showed that YOLOv8 was more precise (less false detection) than Mask R-CNN, particularly in area with challenging lighting and complex backgrounds (e.g., a rectangular box in Fig. 14b). Comparatively, Mask R-CNN exhibited lower performance under these conditions, with the limitations being more apparent in poorly lit areas with complex backgrounds (e.g., Fig. 14c). The segmentation of the branch within the yellow rectangle (Fig. 14d) also highlighted YOLOv8's ability to detect features despite variable lighting conditions created by shadows and hue variations, an area where Mask R-CNN was less robust in segmenting desired objects (Fig. 14e).


![1-s2.0-S258972172400028X-gr13](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/fbd12f2e-b914-48ac-bcab-2890a017bb53-1-s2-0-S258972172400028X-gr13.jpg)
Fig. 13. Example results for multiclass segmentation of trunks (yellow circle) and branches (yellow rectangle) in dormant season orchard images; (a) Original images; (b) YOLOv8 segmentation results; and (c) Mask R-CNN segmentation results. This example showed slightly weaker segmentation performance of Mask R-CNN, qualitatively, compared to YOLOv8. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)


![1-s2.0-S258972172400028X-gr14](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/7acfdca4-22b2-4669-a402-377f5c3c4e6b-1-s2-0-S258972172400028X-gr14.jpg)

Fig. 14. Figures illustrating multiclass segmentation (a) Original Image1; (b) YOLOv8 segmentation (c) Mask R-CNN segmentation; (d) Original image 2; (e) Yolov8 segmentation (f) Mask R-CNN segmentation.

Computational speed is one of the major performance major of these models, particularly when they are used for real-time field applications such as robotic pruning or thinning. The inference times (processing time per image during testing) required for segmenting green fruit and multi-class objects (trunks and branches) with YOLOv8 and Mask R-CNN models are presented in Table 3. It was found that YOLOv8 took only 7.8 ms to complete single-class segmentation and 10.9 ms for multi-class segmentation per test image using Intel Xeon® W-2155 CPU @ 3.30 GHz x20 processor, NVDIA TITAN Xp Collector's edition/PCIe/SSE2 graphics card, 31.1 gigabyte memory, and Ubuntu 16.04 LTS 64-bit operating system. These inference times correspond to inference speeds of approximately 128 FPS and 92 FPS, respectively for single and multi-class segmentations. Comparatively, the inference times for Mask R-CNN was higher at 12.8 ms for single class segmentation, which translates to an inference speed of approximately 78 FPS. For multi-class segmentation, the inference time increased to 15.6 ms for Mask R-CNN, or roughly 64 FPS. This difference in processing time showed suitability of the YOLOv8 for both single and multi-object instance segmentation for real-time application (summarized in Fig. 15).
Table 3. Summary of the performance metrics of YOLOv8 and Mask R-CNN models including precision, recall, mAP@0.5, inference times, and FPS for single and multi-class object segmentation tasks in this study.



![Screenshot 2025-11-28 135457](https://raw.githubusercontent.com/ranzosap/ranjan.sapkota/main/uploads/0ee66367-0d96-483e-abd8-9dfad3f3f336-Screenshot-2025-11-28-135457.png)


Fig. 15. Area under curve (AUC) for the segmentation results of both datasets: Immature green fruit (Apple) dataset on left side; Dormant season orchard dataset on right side. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

According to a most recent study on comparative study of immature green apple detection using machine learning and deep learning models (Liu et al., 2022), Fully Convolutional One-Stage (FCOS) with a ResNet101 RFPN backbone achieved a precision of 81.2%. SSD, utilizing VGG16, had a precision of 69%, while YOLOv3 with Darknet-53 reached 71.3%. Faster-R-CNN and RetinaNet, both employing ResNet101-FPN, achieved precisions of 72.1% and 76.6%, respectively. Lastly, CenterNet, using the Hourglass-104 backbone, recorded a precision of 71.2%. Compared to this result, the precision recorded by both Yolov8 and Mask R-CNN (92.9 and 84.7) in this study is higher. Likewise, other recent studies conducted to detect and segment branch in apple trees such as Unet++ with an accuracy of 72%. Furthermore, studies(Kim et al., 2023) also aimed at similar segmentation tasks, yet their outcomes fall short of the results obtained in our study, where YOLOv8 and Mask R-CNN demonstrated higher precision rates of 90.6% and 81.3% respectively for branch and trunk segmentation.
4.3. Discussion
Mask R-CNN, while demonstrating commendable accuracy in segmenting complex agricultural images, has a slight disadvantage in terms of speed. In this study, for analyzing performances over green apple fruitlets during canopy season and tree trunks and branches during dormant season, Mask R-CNN achieved 78 FPS for single-class and 64 FPS for multi-class segmentation using the System76 workstation. Though this level of inference speed might be sufficient for most of the off-line applications where relatively higher computational capacity could be offered, it may pose challenges in real-time agricultural operations such as automated pruning and rapid decision-making for fertilization with limited computational resources. However, its detailed segmentation capability makes it highly suitable for applications where precision and detailed object delineation is essential.
YOLOv8 stands out for its speed, achieving 128 FPS (1.65)times faster than Mask R-CNN) for single-class and 92 FPS (1.43 time faster than Mask R-CNN) for multi-class segmentation with the same imaging and computational infrastructure used to test Mask R-CNN. Comparatively higher inference speed of this model is particularly advantageous for real-time agricultural tasks as discussed above. Its high precision and recall metrics further emphasize its robust performance across diverse environmental settings including variable light conditions. However, while YOLOv8 offers substantial improvements in speed and accuracy, it may sacrifice some granularity in segmentation compared to two-stage models like Mask R-CNN, which makes it slightly less applicable where minute detail is more critical than processing speed.
YOLOv8's faster inference rates are particularly beneficial for time-sensitive tasks such as automated pruning, especially in low-light conditions, underscoring its superior suitability for operational efficiency in precision agriculture.
In general, however, these findings showed that the two models evaluated in this study could be an effective and efficient tool for developing various precision and automated agricultural tools, with potential applications extending to various crops beyond apples, which will play a crucial role in enhancing crop management and improving crop yield and quality through machine learning. Particularly, YOLOv8 showed good adaptability across different orchard conditions, which is a critical benefit in advancing robust machine learning-based solutions for future innovations in smart farming. The incorporation of machine learning is a key to meet global agricultural sustainability and food security needs.
5. Conclusion
In recent years, there has been increased research, development and adoption of sensing, precision, automation and robotics technologies in agricultural operations, driven by the need to minimize farming inputs including labor and increasing crop yield and quality. This study, through a comprehensive experiment in commercial orchards, provided comparative performance measures of two latest, and most widely used machine learning or deep-learning models (YOLOv8 and Mask R-CNN) for instance segmentation as it relates to their applicability to various crop monitoring and automated canopy and crop-load management tasks (e.g., automated pruning and immature green fruit thinning). Based on the results, the following specific conclusions could be made.
1.
Segmentation Performance in Diverse Conditions: Both YOLOv8 and Mask R-CNN effectively segmented apple tree canopy images from both dormant and early growing seasons. YOLOv8 shows slightly better performance in environments with similar colour features between objects and backgrounds and under varying light intensities.
2.
Single-Class Segmentation (Immature Green Fruit): YOLOv8 outperforms in single-class segmentation of immature green fruits, achieving a precision of 0.92 and a recall of 0.97. In comparison, Mask R-CNN exhibits slightly less effective segmentation capabilities with a precision of 0.84 and a recall of 0.88.
3.
Multi-Class Segmentation (Trunk and Branch Detection): In the detection of both trunk and branches, YOLOv8 displays higher accuracy, achieving the precision and recall metrics of 0.90 and 0.95, respectively. Mask R-CNN achieved lower precision and recall, at 0.81 and 0.83 respectively, indicating reduced effectiveness in multi-class segmentation tasks.
4.
Inference Speed for Multi-Class Segmentation: YOLOv8 maintains robust performance in multi-class segmentation scenarios with a speed of 91.74 FPS. In contrast, Mask R-CNN's slower inference speed of 64.10 FPS suggests limitations in handling applications requiring rapid responses.
6. Future work
Building on the current study, future research could focus on studying evolving capabilities of new object detection models such as YOLOv9released in February 2024 and YOLOv10 released in May 2024, and their accuracy, efficiency and adaptability to agricultural image processing. It is essential to test YOLOv9 and YOLOv10 across diverse agricultural datasets, which include various stages of crop growth, different levels of occlusion, and varying environmental conditions, to evaluate their effectiveness in actual agricultural environments. This study could particularly explore how YOLOv9 and YOLOv10 handle complex detection tasks such as identifying subtle phenotypic changes in crops under challenging light conditions or during different times of the day, situations that are typical in outdoor farming environments. Furthermore, the integration of YOLOv9 and YOLOv10 with Internet of Things (IoT) technologies could be explored to develop advanced systems for real-time monitoring and decision-making in agriculture.
CRediT authorship contribution statement
Ranjan Sapkota: Writing – review & editing, Writing – original draft, Visualization, Validation, Software, Resources, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Dawood Ahmed: Methodology, Formal analysis, Data curation. Manoj Karkee: Writing – review & editing, Supervision, Resources, Project administration, Methodology, Funding acquisition.
Declaration of generative AI and AI-assisted technologies in the writing process
During the preparation of this work, the author used chatGPT in order to correct grammar and language. After using this tool/service, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication.
Declaration of competing interest
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
Acknowledgement
This research is funded by the National Science Foundation and United States Department of Agriculture, National Institute of Food and Agriculture through the “AI Institute for Agriculture” Program (Award No.AWD003473). The authors gratefully acknowledge Dave Allan (Allan Bros., Inc.) for providing access to the orchards during the data collection and field evaluation. Additionally, the authors would like to thank gratefully to Martin Churuvija, Sindhuja Sankaran, Christine Cromar, Bonnie Copeland and Patrick Scharf for their essential support in this project logistics.

References
Afzaal et al., 2021
Usman Afzaal, Bhuwan Bhattarai, Yagya Raj Pandeya, Joonwhoan Lee
An instance segmentation model for strawberry diseases based on mask R-CNN
Sensors, 21 (19) (2021), p. 6565, 10.3390/s21196565
View at publisherView in ScopusGoogle Scholar
Aguiar et al., 2021
André Silva Aguiar, Nuno Namora Monteiro, Filipe Neves, Eduardo J. dos Santos, Solteiro Pires, Daniel Silva, Armando Jorge Sousa, José Boaventura-Cunha
Bringing semantics to the vineyard: An approach on deep learning-based vine trunk detection
Agriculture, 11 (2) (2021), p. 131, 10.3390/agriculture11020131
View at publisherGoogle Scholar
Amogi et al., 2023
Basavaraj R. Amogi, Rakesh Ranjan, Lav R. Khot
Mask R-CNN aided fruit surface temperature monitoring algorithm with edge compute enabled internet of things system for automated apple heat stress management
Informa. Process. Agricult. (2023), 10.1016/j.inpa.2023.12.001
Google Scholar
Arsan and Hameez, 2019
Taner Arsan, Mohammed Muwafaq Noori Hameez
A clustering-based approach for improving the accuracy of UWB sensor-based indoor positioning system
Mob. Inf. Syst., 2019 (2019), pp. 1-13, 10.1155/2019/6372073
Google Scholar
Carranza-García et al., 2020
Manuel Carranza-García, Jesús Torres-Mateo, Pedro Lara-Benítez, Jorge García-Gutiérrez
On the performance of one-stage and two-stage object detectors in autonomous vehicles using camera data
Remote Sens., 13 (1) (2020), p. 89, 10.3390/rs13010089
Google Scholar
Champ et al., 2020
Julien Champ, Adan Mora-Fallas, Hervé Goëau, Erick Mata-Montero, Pierre Bonnet, Alexis Joly
Instance segmentation for the fine detection of crop and weed plants by precision agricultural robots
Appl. Plant Sci., 8 (7) (2020), Article e11373, 10.1002/aps3.11373
View in ScopusGoogle Scholar
Chen et al., 2019
Yuhao Chen, Sriram Baireddy, Enyu Cai, Changye Yang, Edward J. Delp
‘Leaf segmentation by functional modeling’. P. 0
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (2019), 10.1109/CVPRW.2019.00326
Google Scholar
Chen et al., 2021a
Wei Chen, Jingfeng Zhang, Biyu Guo, Qingyu Wei, Zhiyu Zhu
An apple detection method based on des-YOLO v4 algorithm for harvesting robots in complex environment
Math. Probl. Eng., 2021 (2021), pp. 1-12, 10.1155/2021/7351470
View PDF
View articleGoogle Scholar
Chen et al., 2021b
Shuo Chen, Kefei Zhang, Yindi Zhao, Yaqin Sun, Yu Wei Ban, Huifu Zhuang Chen, Xuewei Zhang, Jinxiang Liu, Tao Yang
An approach for Rice bacterial leaf streak disease segmentation and disease severity estimation
Agriculture, 11 (5) (2021), p. 420, 10.3390/agriculture11050420
Google Scholar
Clement et al., 2013
Javier Clement, Nuria Novas, José-Antonio Gazquez, Francisco Manzano-Agugliaro
An active contour computer algorithm for the classification of cucumbers
Comput. Electron. Agric., 92 (2013), pp. 75-81, 10.1016/j.compag.2013.01.006
View PDF
View articleView in ScopusGoogle Scholar
Cong et al., 2022
Peichao Cong, Jiachao Zhou, Shanda Li, Kunfeng Lv, Hao Feng
Citrus tree crown segmentation of orchard spraying robot based on RGB-D image and improved mask R-CNN
Appl. Sci., 13 (1) (2022), p. 164, 10.3390/app13010164
Google Scholar
Coulibaly et al., 2022
Solemane Coulibaly, Bernard Kamsu-Foguem, Dantouma Kamissoko, Daouda Traore
Deep learning for precision agriculture: a bibliometric analysis
Intell. Syst. Appl., 16 (2022), Article 200102, 10.1016/j.iswa.2022.200102
View PDF
View articleView in ScopusGoogle Scholar
Duong-Trung and Duong-Trung, 2024
Hieu Duong-Trung, Nghia Duong-Trung
Integrating YOLOv8-Agri and DeepSORT for advanced motion detection in agriculture and fisheries
EAI Endorsed Trans. Industrial Networks Intelligent Syst., 11 (1) (2024), p. e4, 10.4108/eetinis.v11i1.4618
Google Scholar
Fu et al., 2023
Yuxing Fu, Yuyang Xia, Huiming Zhang, Fu Meng, Yong Wang, Fu Wei, Congju Shen
Skeleton extraction and pruning point identification of jujube tree for dormant pruning using space colonization algorithm
Front. Plant Sci., 13 (2023), p. 1103794, 10.3389/fpls.2022.1103794
View in ScopusGoogle Scholar
Ganesh et al., 2019
P. Ganesh, K. Volle, T.F. Burks, S.S. Mehta
Deep Orange: mask R-CNN based Orange detection and segmentation
IFAC-PapersOnLine, 52 (30) (2019), pp. 70-75, 10.1016/j.ifacol.2019.12.499
View PDF
View articleView in ScopusGoogle Scholar
Gao et al., 2011
Y.A.N. Gao, Jean Francois Mas, Norman Kerle, Jose Antonio Navarrete Pacheco
Optimal region growing segmentation and its effect on classification accuracy
Int. J. Remote Sens., 32 (13) (2011), pp. 3747-3763, 10.1080/01431161003777189
View in ScopusGoogle Scholar
Gao et al., 2022
Fangfang Gao, Wentai Fang, Xiaoming Sun, Wu Zhenchao, Guanao Zhao, Guo Li, Rui Li, Fu Longsheng, Qin Zhang
A novel apple fruit detection and counting methodology based on deep learning and trunk tracking in modern orchard
Comput. Electron. Agric., 197 (2022), Article 107000, 10.1016/j.compag.2022.107000
View PDF
View articleView in ScopusGoogle Scholar
Gentilhomme et al., 2023
Theophile Gentilhomme, Michael Villamizar, Jerome Corre, Jean-Marc Odobez
Towards smart pruning: ViNet, a deep-learning approach for grapevine structure estimation
Comput. Electron. Agric., 207 (2023), Article 107736, 10.1016/j.compag.2023.107736
View PDF
View articleView in ScopusGoogle Scholar
Guadagna et al., 2023
Paolo Guadagna, M. Fernandes, F. Chen, Alessandro Santamaria, Tao Teng, D.G. Tommaso Frioni, Stefano Poni Caldwell, C. Semini, Matteo Gatti
Using deep learning for pruning region detection and plant organ segmentation in dormant spur-pruned grapevines
Precis. Agric., 1–23 (2023), 10.1007/s11119-023-10006-y
Google Scholar
Gupta et al., 2017
Varun Gupta, Namita Sengar, Malay Kishore Dutta, Carlos M. Travieso, B. Jesús
Automated Segmentation of Powdery Mildew Disease from Cherry Leaves Using Image Processing
2017 International Conference and Workshop on Bioinspired Intelligence (IWOBI), IEEE (2017), pp. 1-4, 10.1109/IWOBI.2017.8006454
Google Scholar
Hafiz and Bhat, 2020
Abdul Mueed Hafiz, Ghulam Mohiuddin Bhat
A survey on instance segmentation: state of the art
Int. J. Multimed. Inf. Retr., 9 (3) (2020), pp. 171-189, 10.1007/s13735-020-00195-x
View in ScopusGoogle Scholar
He et al., 2017
Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick
Mask R-Cnn
Proceedings of the IEEE International Conference on Computer Vision (2017), pp. 2961-2969, 10.1109/TPAMI.2018.2844175
Google Scholar
Hoogenboom, 2000
Gerrit Hoogenboom
Contribution of Agrometeorology to the simulation of crop production and its applications
Agric. For. Meteorol., 103 (1–2) (2000), pp. 137-157, 10.1016/S0168-1923(00)00108-8
View PDF
View articleView in ScopusGoogle Scholar
Hu et al., 2021
Chengsong Hu, J. Alex Thomasson, Muthukumar V. Bagavathiannan
A powerful image synthesis and semi-supervised learning pipeline for site-specific weed detection
Comput. Electron. Agric., 190 (2021), Article 106423, 10.1016/j.compag.2021.106423
View PDF
View articleView in ScopusGoogle Scholar
Huang et al., 2021
Zhongqiang Huang, Ping Zhang, Ruigang Liu, Dongxu Li
Immature apple detection method based on improved Yolov3
ASP Trans. Internet Things, 1 (1) (2021), pp. 9-13, 10.52810/TIOT.2021.100028
View in ScopusGoogle Scholar
Hussain et al., 2023
Magni Hussain, Long He, James Schupp, David Lyons, Paul Heinemann
Green fruit segmentation and orientation estimation for robotic green fruit thinning of apples
Comput. Electron. Agric., 207 (2023), Article 107734, 10.1016/j.compag.2023.107734
View PDF
View articleView in ScopusGoogle Scholar
Jabir et al., 2023
Brahim Jabir, Khalid El Moutaouakil, Noureddine Falih
Developing an efficient system with mask R-CNN for agricultural applications
Agris On-Line Pap. Econ. Informa., 15 (1) (2023), pp. 61-72, 10.22004/ag.econ.334659
View in ScopusGoogle Scholar
Jayanthi and Shashikumar, 2019
Muniram Gajendra Jayanthi, Dandinashivara Revanna Shashikumar
Leaf disease segmentation from agricultural images via hybridization of active contour model and OFA
J. Intell. Syst., 29 (1) (2019), pp. 35-52, 10.1515/jisys-2017-0415
Google Scholar
Jha et al., 2019
Kirtan Jha, Aalap Doshi, Poojan Patel, Manan Shah
A comprehensive review on automation in agriculture using artificial intelligence
Artificial Intell. Agriculture, 2 (2019), pp. 1-12, 10.1016/j.aiia.2019.05.004
View PDF
View articleView in ScopusGoogle Scholar
Jia et al., 2020
Weikuan Jia, Yuyu Tian, Rong Luo, Zhonghua Zhang, Jian Lian, Yuanjie Zheng
Detection and segmentation of overlapped fruits based on optimized mask R-CNN application in apple harvesting robot
Comput. Electron. Agric., 172 (2020), Article 105380, 10.1016/j.compag.2020.105380
View PDF
View articleView in ScopusGoogle Scholar
Jia et al., 2021
Weikuan Jia, Zhonghua Zhang, Wenjing Shao, Sujuan Hou, Ze Ji, Guoliang Liu, Xiang Yin
FoveaMask: a fast and accurate deep learning model for green fruit instance segmentation
Comput. Electron. Agric., 191 (2021), Article 106488, 10.1016/j.compag.2021.106488
View PDF
View articleView in ScopusGoogle Scholar
Jia et al., 2022
Weikuan Jia, Jinmeng Wei, Qi Zhang, Ningning Pan, Yi Niu, Xiang Yin, Yanhui Ding, Xinting Ge
Accurate segmentation of green fruit based on optimized mask RCNN application in complex orchard
Front. Plant Sci., 13 (2022), Article 955256, 10.3389/fpls.2022.955256
View in ScopusGoogle Scholar
Jothiaruna et al., 2019
N. Jothiaruna, K. Joseph Abraham Sundar, B. Karthikeyan
A segmentation method for disease spot images incorporating chrominance in comprehensive color feature and region growing
Comput. Electron. Agric., 165 (2019), Article 104934, 10.1016/j.compag.2019.104934
View PDF
View articleView in ScopusGoogle Scholar
Karthikeyan et al., 2023
M. Karthikeyan, T.S. Subashini, R. Srinivasan, C. Santhanakrishnan, A. Ahilan
YOLOAPPLE: augment Yolov3 deep learning algorithm for apple fruit quality detection
SIViP, 1–10 (2023), 10.1007/s11760-023-02710-z
Google Scholar
Khirade et al., 2015
Khirade, D. Sachin, B. Patil. Amit
Plant disease detection using image processing, IEEE (2015), pp. 768-771
https://doi.org/10.1109/ICCUBEA.2015.153
CrossrefView in ScopusGoogle Scholar
Kim et al., 2023
Dong-Hyeon Kim, Chi-Ung Ko, Dong-Geun Kim, Jin-Taek Kang, Jeong-Mook Park, Hyung-Ju Cho
Automated segmentation of individual tree structures using deep learning over LiDAR point cloud data
Forests, 14 (6) (2023), p. 1159, 10.3390/f14061159
Google Scholar
Kok et al., 2023
Eugene Kok, Xing Wang, Chao Chen
Obscured tree branches segmentation and 3D reconstruction using deep learning and geometrical constraints
Comput. Electron. Agric., 210 (2023), Article 107884, 10.1016/j.compag.2023.107884
View PDF
View articleView in ScopusGoogle Scholar
Kuznetsova et al., 2021
Anna Kuznetsova, Tatiana Maleva, Vladimir Soloviev
YOLOv5 versus YOLOv3 for apple detection
Cyber-Physical Systems: Modelling and Intelligent Control, Springer (2021), pp. 349-358, 10.1007/978-3-030-66077-2_28
View in ScopusGoogle Scholar
Li et al., 2022
He Li, Changle Guo, Zishang Yang, Jiajun Chai, Yunhui Shi, Jiawei Liu, Kaifei Zhang, Daoqi Liu, Xu. Yufei
Design of Field Real-Time Target Spraying System Based on improved YOLOv5
Front. Plant Sci., 13 (2022), p. 1072631, 10.3389/fpls.2022.1072631
View in ScopusGoogle Scholar
Li et al., 2023
Yiting Li, Qingsong Fan, Haisong Huang, Zhenggong Han, Gu. Qiang
A modified YOLOv8 detection network for UAV aerial image recognition
Drones, 7 (5) (2023), p. 304, 10.3390/drones7050304
Google Scholar
Lin et al., 2020
Tu-Liang Lin, Hong-Yi Chang, Kai-Hong Chen
The Pest and disease identification in the growth of sweet peppers using faster R-CNN and mask R-CNN
J. Internet Technol., 21 (2) (2020), pp. 605-614, 10.1109/ICCE-TW46550.2019.8991893
View in ScopusGoogle Scholar
Lin et al., 2021
Guichao Lin, Yunchao Tang, Xiangjun Zou, Chenglin Wang
Three-dimensional reconstruction of guava fruits and branches using instance segmentation and geometry analysis
Comput. Electron. Agric., 184 (2021), Article 106107, 10.1016/j.compag.2021.106107
View PDF
View articleView in ScopusGoogle Scholar
Lippi et al., 2021
Martina Lippi, Niccolò Bonucci, Renzo Fabrizio Carpio, Mario Contarini, Stefano Speranza, Andrea Gasparri
A Yolo-Based Pest Detection System for Precision Agriculture
2021 29th Mediterranean Conference on Control and Automation (MED), IEEE (2021), pp. 342-347, 10.1109/MED51440.2021.9480344
View in ScopusGoogle Scholar
Liu and Wang, 2020
Jun Liu, Xuewei Wang
Tomato diseases and pests detection based on improved yolo V3 convolutional neural network
Front. Plant Sci., 11 (2020), p. 898, 10.3389/fpls.2020.00898
View in ScopusGoogle Scholar
Liu et al., 2021
Yikun Liu, Gongping Yang, Yuwen Huang, Yilong Yin
SE-mask R-CNN: An improved mask R-CNN for apple detection and segmentation
J. Intell. Fuzzy Syst., 41 (6) (2021), pp. 6715-6725, 10.3233/JIFS-210597
View in ScopusGoogle Scholar
Liu et al., 2022
Mengyuan Liu, Weikuan Jia, Zhifen Wang, Yi Niu, Xinbo Yang, Chengzhi Ruan
An accurate detection and segmentation model of obscured green fruits
Comput. Electron. Agric., 197 (2022), Article 106984, 10.1016/j.compag.2022.106984
View PDF
View articleView in ScopusGoogle Scholar
Lu et al., 2022
Shenglian Lu, Wenkang Chen, Xin Zhang, Manoj Karkee
Canopy-attention-YOLOv4-based immature/mature apple fruit detection on dense-foliage tree architectures for early crop load estimation
Comput. Electron. Agric., 193 (2022), Article 106696, 10.1016/j.compag.2022.106696
View PDF
View articleView in ScopusGoogle Scholar
Lüling et al., 2021
Nils Lüling, David Reiser, Hans W. Griepentrog
Volume and leaf area calculation of cabbage with a neural network-based instance segmentation
Precision agriculture’21, Wageningen Academic Publishers (2021), pp. 2719-2745, 10.3920/978-90-8686-916-9_86
Google Scholar
Lv et al., 2022
Jidong Lv, Xu Hao, Ying Han, Lu Wenbin, Xu Liming, Hailong Rong, Biao Yang, Ling Zou, Zhenghua Ma
A visual identification method for the apple growth forms in the orchard
Comput. Electron. Agric., 197 (2022), Article 106954, 10.1016/j.compag.2022.106954
View PDF
View articleView in ScopusGoogle Scholar
Ma et al., 2017
Juncheng Ma, Du Keming, Lingxian Zhang, Feixiang Zheng, Jinxiang Chu, Zhongfu Sun
A segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing
Comput. Electron. Agric., 142 (2017), pp. 110-117, 10.1016/j.compag.2017.08.023
View PDF
View articleGoogle Scholar
Ma et al., 2021
Baojian Ma, Du Jing, Le Wang, Huanyu Jiang, Mingchuan Zhou
Automatic branch detection of jujube trees based on 3D reconstruction for dormant pruning using the deep learning-based method
Comput. Electron. Agric., 190 (2021), Article 106484, 10.1016/j.compag.2021.106484
View PDF
View articleView in ScopusGoogle Scholar
Ma et al., 2023
Li Ma, Liya Zhao, Zixuan Wang, Jian Zhang, Guifen Chen
Detection and counting of small target apples under complicated environments by using improved YOLOv7-tiny
Agronomy, 13 (5) (2023), p. 1419, 10.3390/agronomy13051419
View in ScopusGoogle Scholar
Maji et al., 2022
Arpan K. Maji, Sudeep Marwaha, Sudhir Kumar, Alka Arora, Viswanathan Chinnusamy, Shahnawazul Islam
SlypNet: spikelet-based yield prediction of wheat using advanced plant phenotyping and computer vision techniques
Front. Plant Sci., 13 (2022), Article 889853, 10.3389/fpls.2022.889853
View in ScopusGoogle Scholar
Mu et al., 2023
Xinyang Mu, Long He, Paul Heinemann, James Schupp, Manoj Karkee
Mask R-CNN based apple flower detection and king flower identification for precision pollination
Smart Agricult. Technol., 4 (2023), Article 100151, 10.1016/j.atech.2022.100151
View PDF
View articleView in ScopusGoogle Scholar
Niu et al., 2016
Niu, Chong, Han Li, Yuguang Niu, Zengchan Zhou, Yunlong Bu, and Wengang Zheng. "Segmentation of cotton leaves based on improved watershed algorithm." In Computer and Computing Technologies in Agriculture IX: 9th IFIP WG 5.14 International Conference, CCTA 2015, Beijing, China, September 27-30, 2015, Revised Selected Papers, Part I 9, pp. 425-436. Springer International Publishing, 2016. https://doi.org/10.1007/978-3-319-48357-3_41
Google Scholar
Ngugi et al., 2021
Lawrence C. Ngugi, Moataz Abelwahab, Mohammed Abo-Zahhad
Recent advances in image processing techniques for automated leaf Pest and disease recognition–a review
Informa. Process. Agricult., 8 (1) (2021), pp. 27-51, 10.1016/j.inpa.2020.04.004
View PDF
View articleView in ScopusGoogle Scholar
Osorio et al., 2020
Kavir Osorio, Andrés Puerto, Cesar Pedraza, David Jamaica, Leonardo Rodríguez
A deep learning approach for weed detection in lettuce crops using multispectral images
AgriEngineering, 2 (3) (2020), pp. 471-488, 10.3390/agriengineering2030032
View in ScopusGoogle Scholar
Pham and Lee, 2015
Van Huy Pham, Byung Ryong Lee
An image segmentation approach for fruit defect detection using K-means clustering and graph-based algorithm
Vietnam J. Comput. Sci., 2 (2015), pp. 25-33, 10.1007/s40595-014-0028-3
View in ScopusGoogle Scholar
Prabhu and Rani, 2021
Akshatha Prabhu, N. Shobha Rani
Semiautomated segmentation model to extract fruit images from trees
2021 International Conference on Intelligent Technologies (CONIT), IEEE (2021), pp. 1-13, 10.1109/CONIT51480.2021.9498501
Google Scholar
Qu et al., 2023
Xiaofeng Qu, Jiajun Wang, Xiaoling Wang, Hu Yike, Tuocheng Zeng, Tianwen Tan
Gravelly soil uniformity identification based on the optimized mask R-CNN model
Expert Syst. Appl., 212 (2023), Article 118837, 10.1016/j.eswa.2022.118837
View PDF
View articleView in ScopusGoogle Scholar
Rashid et al., 2023
Javed Rashid, Imran Khan, Ghulam Ali, Fahad Alturise, Tamim Alkhalifah
Real-time multiple guava leaf disease detection from a single leaf using hybrid deep learning technique
Comput. Mater. Continua, 74 (1) (2023), 10.32604/cmc.2023.032005
Google Scholar
Redmon et al., 2016
Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi
You only look once: unified, real-time object detection
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016), pp. 779-788
http://pjreddie.com/yolo/
Google Scholar
Rehman et al., 2021
Zia Ur Rehman, Muhammad Attique Khan, Fawad Ahmed, Robertas Damaševičius, Syed Rameez Naqvi, Wasif Nisar, Kashif Javed
Recognizing apple leaf diseases using a novel parallel real-time processing framework based on MASK RCNN and transfer learning: An application for smart agriculture
IET Image Process., 15 (10) (2021), pp. 2157-2168, 10.1049/ipr2.12183
View in ScopusGoogle Scholar
Safonova et al., 2021
Anastasiia Safonova, Emilio Guirado, Yuriy Maglinets, Domingo Alcaraz-Segura, Siham Tabik
Olive tree biovolume from UAV multi-resolution image segmentation with mask R-CNN
Sensors, 21 (5) (2021), p. 1617, 10.3390/s21051617
Google Scholar
Seol et al., 2022
Jaehwi Seol, Jeongeun Kim, Hyoung Il Son
Field evaluations of a deep learning-based intelligent spraying robot with flow control for pear orchards
Precis. Agric., 23 (2) (2022), pp. 712-732, 10.1007/s11119-021-09856-1
View in ScopusGoogle Scholar
Siddique et al., 2021
Nahian Siddique, Sidike Paheding, Colin P. Elkin, Vijay Devabhaktuni
U-net and its variants for medical image segmentation: a review of theory and applications
Ieee Access, 9 (2021), pp. 82031-82057, 10.1109/ACCESS.2021.3086020
View in ScopusGoogle Scholar
Soviany and Ionescu, 2018
Petru Soviany, Radu Tudor Ionescu
Optimizing the Trade-off between Single-Stage and Two-Stage Deep Object Detectors Using Image Difficulty Prediction
2018 20th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC), IEEE (2018), pp. 209-214, 10.1109/SYNASC.2018.00041
View in ScopusGoogle Scholar
Su et al., 2022
Fei Su, Yanping Zhao, Yanxia Shi, Dong Zhao, Guanghui Wang, Yinfa Yan, Zu Linlu, Siyuan Chang
Tree trunk and obstacle detection in apple orchard based on improved YOLOv5s model
Agronomy, 12 (10) (2022), p. 2427, 10.3390/agronomy12102427
View in ScopusGoogle Scholar
Sun et al., 2019
Sashuang Sun, Mei Jiang, Dongjian He, Yan Long, Huaibo Song
Recognition of green apples in an orchard environment by combining the GrabCut model and Ncut algorithm
Biosyst. Eng., 187 (2019), pp. 201-213, 10.1016/j.biosystemseng.2019.09.006
View PDF
View articleView in ScopusGoogle Scholar
Sun et al., 2022
Meili Sun, Xu Liancheng, Rong Luo, Lu Yuqi, Weikuan Jia
GHFormer-net: towards more accurate small green apple/Begonia fruit detection in the nighttime
J. King Saud Univ. Comp. Informa. Sci., 34 (7) (2022), pp. 4421-4432, 10.1016/j.jksuci.2022.05.005
View PDF
View articleView in ScopusGoogle Scholar
Tang et al., 2023
Can Tang, Du Chen, Xin Wang, Xindong Ni, Yehong Liu, Xu Yihao Liu, Mao, and Shumao Wang.
A fine recognition method of strawberry ripeness combining mask R-CNN and region segmentation
Front. Plant Sci., 14 (2023), p. 1211830, 10.3389/fpls.2023.1211830
View in ScopusGoogle Scholar
Tian et al., 2019a
Kai Tian, Jiuhao Li, Jiefeng Zeng, Asenso Evans, Lina Zhang
Segmentation of tomato leaf images based on adaptive clustering number of K-means algorithm
Comput. Electron. Agric., 165 (2019), Article 104962, 10.1016/j.compag.2019.104962
View PDF
View articleView in ScopusGoogle Scholar
Tian et al., 2019b
Yunong Tian, Guodong Yang, Zhe Wang, Hao Wang, En Li, Zize Liang
Apple detection during different growth stages in orchards using the improved YOLO-V3 model
Comput. Electron. Agric., 157 (2019), pp. 417-426, 10.1016/j.biosystemseng.2020.03.008
View PDF
View articleView in ScopusGoogle Scholar
Tian et al., 2020
Yunong Tian, Guodong Yang, Zhe Wang, En Li, Zize Liang
Instance segmentation of apple flowers using the improved mask R–CNN model
Biosyst. Eng., 193 (2020), pp. 264-278, 10.1016/j.biosystemseng.2020.03.008
View PDF
View articleView in ScopusGoogle Scholar
Tong et al., 2022
Siyuan Tong, Yang Yue, Wenbin Li, Yaxiong Wang, Feng Kang, Chao Feng
Branch identification and junction points location for apple trees based on deep learning
Remote Sens., 14 (18) (2022), p. 4495, 10.3390/rs14184495
View in ScopusGoogle Scholar
Wang and Liu, 2024
Xuewei Wang, Jun Liu
Vegetable disease detection using an improved YOLOv8 algorithm in the greenhouse plant environment
Sci. Rep., 14 (1) (2024), p. 4261, 10.1038/s41598-024-54540-9
View in ScopusGoogle Scholar
Wang et al., 2021
Shijie Wang, Guiling Sun, Bowen Zheng, Du. Yawen
A crop image segmentation and extraction algorithm based on mask RCNN
Entropy, 23 (9) (2021), p. 1160, 10.3390/e23091160
View in ScopusGoogle Scholar
Wang et al., 2022
Qifan Wang, Man Cheng, Shuo Huang, Zhenjiang Cai, Jinlin Zhang, Hongbo Yuan
A deep learning approach incorporating YOLO v5 and attention mechanisms for field real-time detection of the invasive weed solanum Rostratum Dunal seedlings
Comput. Electron. Agric., 199 (2022), Article 107194, 10.1016/j.compag.2022.107194
View PDF
View articleView in ScopusGoogle Scholar
Wan and Sotirios, 2020
Shaohua Wan, Goudos Sotirios
Faster R-CNN for multi-class fruit detection using a robotic vision system
Computer Networks, 168 (2020), p. 107036
https://doi.org/10.1016/j.comnet.2019.107036
View PDF
View articleView in ScopusGoogle Scholar
Wang et al., 2023a
Gang Wang, Yanfei Chen, Pei An, Hanyu Hong, Hu Jinghu, Tiange Huang
UAV-YOLOv8: a small-object-detection model based on improved YOLOv8 for UAV aerial photography scenarios
Sensors, 23 (16) (2023), p. 7190, 10.3390/s23167190
View in ScopusGoogle Scholar
Wang et al., 2023b
Tao Wang, Wu Kunming Zhang, Ruiqing Wang Zhang, Shengmin Wan, Yuan Rao, Zhaohui Jiang, Gu. Lichuan
Tea picking point detection and location based on mask-RCNN
Informa. Process. Agricult., 10 (2) (2023), pp. 267-275, 10.1016/j.inpa.2021.12.004
View PDF
View articleView in ScopusGoogle Scholar
Wei et al., 2022
Jinmeng Wei, Yanhui Ding, Jie Liu, Muhammad Zakir Ullah, Xiang Yin, Weikuan Jia
Novel green-fruit detection algorithm based on D2D framework
Int. J. Agricultu. Biol. Eng., 15 (1) (2022), pp. 251-259, 10.25165/j.ijabe.20221501.6943
View in ScopusGoogle Scholar
Wu et al., 2021
Lin Wu, Jie Ma, Yuehua Zhao, Hong Liu
Apple detection in complex scene using the improved YOLOv4 model
Agronomy, 11 (3) (2021), p. 476, 10.3390/agronomy11030476
View in ScopusGoogle Scholar
Xiang et al., 2022
Rong Xiang, Maochen Zhang, Jielan Zhang
Recognition for stems of tomato plants at night based on a hybrid joint neural network
Agriculture, 12 (6) (2022), p. 743, 10.3390/agriculture12060743
View in ScopusGoogle Scholar
Xie et al., 2021
Shuangyu Xie, Hu Chengsong, Muthukumar Bagavathiannan, Dezhen Song
Toward robotic weed control: detection of Nutsedge weed in Bermudagrass turf using inaccurate and insufficient training data
IEEE Robot. Automation Letters, 6 (4) (2021), pp. 7365-7372, 10.1109/LRA.2021.3098012
View in ScopusGoogle Scholar
Xu et al., 2020
Beibei Xu, Wensheng Wang, Greg Falzon, Paul Kwan, Leifeng Guo, Zhiguo Sun, Chunlei Li
Livestock classification and counting in quadcopter aerial images using mask R-CNN
Int. J. Remote Sens., 41 (21) (2020), pp. 8121-8142, 10.1080/01431161.2020.1734245
View in ScopusGoogle Scholar
Yang et al., 2023
Guoliang Yang, Jixiang Wang, Ziling Nie, Hao Yang, Yu. Shuaiying
A lightweight YOLOv8 tomato detection algorithm combining feature enhancement and attention
Agronomy, 13 (7) (2023), p. 1824, 10.3390/agronomy13071824
View in ScopusGoogle Scholar
Yu et al., 2019
Yang Yu, Kailiang Zhang, Li Yang, Dongxing Zhang
Fruit detection for strawberry harvesting robot in non-structural environment based on mask-RCNN
Comput. Electron. Agric., 163 (2019), Article 104846, 10.1016/j.compag.2019.06.001
View PDF
View articleView in ScopusGoogle Scholar
Yu et al., 2022
Chuang Yu, Hu Zhuhua, Ruoqing Li, Xin Xia, Yaochi Zhao, Xiang Fan, Yong Bai
Segmentation and density statistics of Mariculture cages from remote sensing images using mask R-CNN
Informa. Process. Agricult., 9 (3) (2022), pp. 417-430, 10.1016/j.inpa.2021.04.013
View PDF
View articleView in ScopusGoogle Scholar
Yue et al., 2023
Xiang Yue, Kai Qi, Xinyi Na, Yang Zhang, Yanhua Liu, Cuihong Liu
Improved YOLOv8-Seg network for instance segmentation of healthy and diseased tomato plants in the growth stage
Agriculture, 13 (8) (2023), p. 1643, 10.3390/agriculture13081643
View in ScopusGoogle Scholar
Zeng et al., 2009
Qingbing Zeng, Yubin Miao, Chengliang Liu, Shiping Wang
Algorithm based on marker-controlled watershed transform for overlapping plant fruit segmentation
Opt. Eng., 48 (2) (2009), p. 27201, 10.1117/1.3076212
Google Scholar
Zhang et al., 2018
Jing Zhang, Long He, Manoj Karkee, Qin Zhang, Xin Zhang, Zongmei Gao
Branch detection for apple trees trained in Fruiting Wall architecture using depth features and regions-convolutional neural network (R-CNN)
Comput. Electron. Agric., 155 (2018), pp. 386-393, 10.1016/j.compag.2018.10.029
View PDF
View articleView in ScopusGoogle Scholar
Zhang et al., 2020a
Lili Zhang, Wu Jisen, Yu Fan, Hongmin Gao, Yehong Shao
An efficient building extraction method from high spatial resolution remote sensing images based on improved mask R-CNN
Sensors, 20 (5) (2020), p. 1465, 10.3390/s20051465
View in ScopusGoogle Scholar
Zhang et al., 2020b
Qian Zhang, Yeqi Liu, Chuanyang Gong, Yingyi Chen, Yu. Huihui
Applications of deep learning for dense scenes analysis in agriculture: a review
Sensors, 20 (5) (2020), p. 1520, 10.3390/s20051520
View in ScopusGoogle Scholar
Zhang et al., 2022
Chenxi Zhang, Feng Kang, Yaxiong Wang
An improved apple object detection method based on lightweight YOLOv4 in complex backgrounds
Remote Sens., 14 (17) (2022), p. 4150, 10.3390/rs14174150
View in ScopusGoogle Scholar
Zhang et al., 2023
Lijuan Zhang, Gongcheng Ding, Chaoran Li, Dongming Li
DCF-Yolov8: An improved algorithm for aggregating low-level features to detect agricultural pests and diseases
Agronomy, 13 (8) (2023), p. 2012, 10.3390/agronomy13082012
Google Scholar
Zhao et al., 2018
Tiebiao Zhao, Yonghuan Yang, Haoyu Niu, Dong Wang, YangQuan Chen
Comparing U-net convolutional network with mask R-CNN in the performances of pomegranate tree canopy segmentation
Multispectral, Hyperspectral, and Ultraspectral Remote Sensing Technology, Techniques and Applications, Vol. 10780, SPIE (2018), pp. 210-218, 10.1117/12.2325570
Google Scholar
Zu et al., 2021
Linlu Zu, Yanping Zhao, Jiuqin Liu, Su Fei, Yan Zhang, Pingzeng Liu
Detection and segmentation of mature green tomatoes based on mask R-CNN with automatic image acquisition approach
Sensors, 21 (23) (2021), p. 7842, 10.3390/s21237842
View in ScopusGoogle Scholar


    