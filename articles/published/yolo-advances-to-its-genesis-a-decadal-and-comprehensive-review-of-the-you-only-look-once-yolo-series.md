
    title: "YOLO advances to its genesis: a decadal and comprehensive review of the You Only Look Once (YOLO) series"
    authors: ["Ranjan Sapkota", "Marco Flore-Calero", "Rizwan Qureshi", "Chetan Badgujar", "Upesh Nepal", "Alwin Pauloso", "Manoh karkee", "Suraj Chaudhary"]
    year: 2025
    month: 11
    publication_venue: "Springer Nature Link"
    abstract: "This review systematically examines the progression of the You Only Look Once (YOLO) object detection algorithms from YOLOv1 to the recently unveiled YOLOv12. Employing a reverse chronological analysis, this study examines the advancements introduced by YOLO algorithms, beginning with YOLOv12 and progressing through YOLO11 (or YOLOv11), YOLOv10, YOLOv9, YOLOv8, and subsequent versions to explore each version’s contributions to enhancing speed, detection accuracy, and computational efficiency in real-time object detection. Additionally, this study reviews the alternative versions derived from YOLO architectural advancements of YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and Gold-YOLO. Moreover, the study highlights the transformative impact of YOLO models across five critical application areas: autonomous vehicles and traffic safety, healthcare and medical imaging, industrial manufacturing, surveillance and security, and agriculture. By detailing the incremental technological advancements in subsequent YOLO versions, this review chronicles the evolution of YOLO, and discusses the challenges and limitations in each of the earlier versions. The evolution signifies a path towards integrating YOLO with multimodal, context-aware, and Artificial General Intelligence (AGI) systems for the next YOLO decade, promising significant implications for future developments in AI-driven applications."
    tags: []
    published: true
    featured: false
    doi: "https://link.springer.com/article/10.1007/s10462-025-11253-3#Tab1"
    pdf_url: "https://link.springer.com/article/10.1007/s10462-025-11253-3#Tab1"
    code_url: "https://link.springer.com/article/10.1007/s10462-025-11253-3#Tab1"
    dataset_url: ""
    github_link: "https://github.com/ranzosap/ranjan.sapkota/blob/main/articles/published/yolo-advances-to-its-genesis-a-decadal-and-comprehensive-review-of-the-you-only-look-once-yolo-series.md"
    

    1 Introduction
Object detection is a critical component of computer vision systems, which enables automated systems to identify and locate objects of interest within images or video frames (Liu et al. 2020; Badgujar et al. 2024; Ahmad and Rahimi 2022; Gheorghe et al. 2024; Arkin et al. 2023). Real-time object detection has become integral to numerous applications requiring real- and near-real-time analysis, monitoring and interaction with dynamic environments such as agriculture, transportation, education, and health-care (Fernandez et al. 2016; Wang et al. 2018; Ren et al. 2015; Tang et al. 2024; Chen and Guan 2022; Ragab et al. 2024). For instance, real-time object detection is the foundational technology for the success of autonomous vehicles and robotic systems (Flippo et al. 2023; Malligere Shivanna and Guo 2024; Flores-Calero et al. 2024), allowing the system to quickly recognize and track different objects of interest such as vehicles, pedestrians, bicycles, and other obstacles, enhancing navigational safety and efficiency (Guerrero-Ibáñez et al. 2018; Shoman et al. 2024a). The utility of object recognition extends beyond vehicular applications, and is also pivotal in action recognition within video sequences, useful in digital surveillance, monitoring, sports analysis, cityscapes (Hnewa and Radha 2023) and human–machine interaction (Hussain and Zeadally 2018; Fernandez et al. 2016; Shoman et al. 2024b). These areas benefit from the capability to analyze and respond to situational dynamics in real time, illustrating its broad applicability, acceptance, and impact. However, the problem of object detection involves several challenges:

Complexity of real-world environments: Real-world environments/scenes are highly variable and unpredictable. Objects can appear in various orientations, scales, distances and lighting conditions, making it difficult for a detection algorithm to generalize and maintain accuracy in real time (Kaushal et al. 2018).

Illumination factors: Illumination plays a crucial role in object detection, as factors like lighting intensity, direction, shadows, and glare can significantly affect performance (Xiang et al. 2014; Xiao et al. 2020). Non-uniform or low light, color temperature changes, and dynamic lighting variations can obscure object features or cause false detections. Solutions include controlled lighting setups, preprocessing techniques like normalization and color correction, and training models with diverse, augmented datasets to enhance robustness (Seoni et al. 2024).

Occlusions and clutter: Objects may be partially or fully obscured by other objects, creating cluttered scenes that result in incomplete information, which requires careful interpretation for accurate analysis (Khan and Shah 2008; Mostafa et al. 2022).

Speed and efficiency: Many applications necessitate rapid processing of visual data to enable timely decision-making. This requires detection algorithms to achieve a balance between high accuracy and low latency, ensuring that the systems can deliver efficient and reliable results in real- or near-real-time scenarios, such as autonomous vehicles and traffic safety, healthcare and medical imaging, industrial manufacturing, security and surveillance and agricultural automation (Gupta et al. 2021).

Addressing these challenges, as discussed below, required innovative techniques, which conventionally relied on hand-crafted features and classical machine learning methods. Later, the focus shifted towards automated feature learning and end-to-end deep learning methods.

1.1 Traditional object detection approaches
Before the advent of deep learning, object detection relied on a combination of hand-crafted features and machine learning classifiers (Zou et al. 2023). Some of the notable traditional methods include:

Correlation filters: Used to detect objects by correlating a filter with the image, such as matching templates (Park et al. 2019). These approaches struggle with variations in the appearance of objects and lighting conditions (Liu et al. 2021).

Sliding window approach: This method involves moving a fixed-size window across the image and applying a classifier to each window to determine whether it contains an object (Teutsch and Kruger 2015). However, it struggles with varying object sizes and aspect ratios, which can lead to inaccurate detections and a high computational cost due to the exhaustive search involved.

Viola–Jones detector: The Viola-Jones detector, introduced in 2001, uses Haar-like features (Lienhart and Maydt 2002) and a cascade of AdaBoost trained classifiers (Jun-Feng and Yu-Pin 2009) to detect objects in images efficiently (Li et al. 2012).

Supporting these methods are various hand-crafted feature extraction techniques, including:

Gabor features: Extracted texture features using Gabor filters, which are effective for texture representation but can be computationally intensive (Hu et al. 2020).

Histogram of oriented gradients (HOG): Captures edge or gradient structures that characterize the shape of objects, typically combined with Support Vector Machines (SVM) for classification (Surasak et al. 2018).

Local binary patterns (LBP): Utilizes pixel intensity comparisons to form a binary pattern, used in texture classification and face recognition (Karis et al. 2016).

Haar-like features: These features consider adjacent rectangular regions in a detection window, sum up the pixel intensities in each region, and calculate the difference between the sums. This difference is then used to categorize sub-regions in images (Mita et al. 2005).

DPM (Deformable part models): DPM (Yan et al. 2014) represents objects as a collection of deformable parts arranged in a spatial structure. It proved particularly effective for detecting objects under occlusions, pose variations, and cluttered backgrounds.

SIFT: Scale-Invariant Feature Transform (SIFT) is a robust method for detecting and describing local features in images (Piccinini et al. 2012). Beyond feature extraction, it has been effectively used for object detection by matching keypoints between input images and reference templates, leveraging its invariance to scale, rotation, and illumination changes.

SURF: speeded-up robust features A faster alternative to SIFT, SURF detects and describes features using an efficient Hessian matrix approximation, making it suitable for real-time object detection tasks (Li and Zhang 2013).

1.1.1 Classification techniques used in traditional object detection methods
Some of the most commonly employed classification methods for these traditional object detectors include Support Vector Machine (SVM), statistical classifiers (e.g., Bayesian Classifier) and ensemble methods (e.g. Adaboost, Random Forest) and Multilayer Perceptrons (MLP) Neural Networks (Chiu et al. 2020; Mienye and Sun 2022).

These traditional methods in early computer vision systems, reliant on hand-crafted features and classical classifiers, offered moderate success under controlled conditions but struggled with robustness and generalization in diverse real-world scenarios, lacking the accuracy achieved by modern deep learning techniques (Xiang et al. 2014). Figure 1 shows the historical development of computer vision systems emphasizing how object detection algorithms evolved.

1.2 Emergence of convolutional neural networks
After 2010, the performance of handcrafted features plateaued, leading to saturation in object detection research. However, in 2012, the world witnessed the emergence of convolutional neural networks (CNNs), marking a significant turning point in the field (Krizhevsky et al. 2012; Xie et al. 2021; Tang and Yuan 2015; Zhiqiang and Jun 2017). As a deep convolutional

network, CNNs are able to learn robust and high-level feature representations of an image, and are particularly effective because:

Hierarchical feature learning: CNNs learn to extract low-level features (e.g., edges, textures) in early layers and high-level features (e.g., object parts, shapes) in deeper layers, facilitating robust object representation (Li et al. 2020).

Spatial invariance: Convolutional layers enable CNNs to recognize objects regardless of their position within the image, enhancing detection robustness (Crawford and Pineau 2019).

Scalability and generalizability: CNNs can be scaled to handle larger datasets and more complex models, improving performance and robustness on a wide range of tasks and application environments  (Tan et al. 2020).

However, CNNs can not be directly applied to the object detection task, due to varying numbers of objects, varying sizes, aspect ratios, and orientation. CNNs were primarily designed for image classification, meaning they output a single label for the entire image (Arkin et al. 2023). Whereas object detection tasks require not only classifying the object but also localizing it in the image, i.e., identifying the position of the object through bounding boxes.

1.3 Timeline of object detection paradigms and the evolution of YOLO models


![10462_2025_11253_Fig1_HTML](/uploads/51606eeb-9417-47c6-8bc1-47b385cf171f-10462_2025_11253_Fig1_HTML.webp)

One of the earliest deep learning-based object detectors was R-CNN, introduced in 2014 by Girshick et al. (2014). It marked a pivotal milestone in the development of detection models, breaking the stagnation in object detection by introducing Regions with CNN features (R-CNN). This groundbreaking approach revolutionized the field, sparking rapid advancements and accelerating the evolution of object detection at an unprecedented pace.

The idea behind R-CNN is simple, it uses the selective search algorithm to generate about 2000 region proposals, which are then processed by a CNN to extract features (Girshick et al. 2014). Finally, linear SVM classifiers are utilized to detect objects within each region and identify their respective categories. While R-CNN achieved significant progress, it has notable drawbacks: the redundant feature computations across a large number of overlapping proposals (over 2,000 boxes per image) result in extremely slow detection speeds, taking 14 s per image even with GPU acceleration (Xie et al. 2021).

After that, Fast R-CNN, introduced in 2015, improved object detection by addressing the redundant feature computations across numerous overlapping proposals (Bhat et al. 2023). It integrated region proposal feature extraction and classification into a single pass, significantly enhancing efficiency and speed compared to previous methods like R-CNN (Girshick 2015). Building on this, Faster R-CNN advanced the approach further by introducing Region Proposal Networks (RPNs), enabling end-to-end training. This innovation eliminated the reliance on selective search, reducing computational complexity and streamlining the pipeline, thus allowing for faster and more accurate object detection without the need for external proposal generation (Ren et al. 2015; Mostafa et al. 2022).

Later, the Single Shot Multibox Detector (SSD) (Liu et al. 2016), introduced at the beginning of 2016, discretizes bounding boxes into predefined default boxes of various scales and aspect ratios. It predicts object scores and adjusts box shapes accordingly. By leveraging multi-scale feature maps, SSD handles objects of different sizes effectively. Unlike earlier methods, it eliminates the need for a separate proposal generation step, simplifying the training process and improving performance, particularly in detecting small objects.

These breakthroughs laid the foundation for numerous subsequent advancements in the field. Over the years, several other detectors have been developed, and by 2024, the YOLO series has become a dominant force in the domain, continuously evolving to improve speed, accuracy, and efficiency in real-time object detection tasks.

Fig. 1 presents a chronological overview of deep learning-based detectors, categorized into YOLO and others. Traditional detectors, shown on the right side, as well as Transformer-based detectors, are included for visual comparison.

1.4 You Only Look Once approach
The “You Only Look Once” (YOLO) object detection algorithm was first introduced by Redmon et al. (2016) in 2015, revolutionized real-time object detection by combining region proposal and classification into a single neural network, significantly reducing computation time. YOLO’s unified architecture divides the image into a grid, predicting bounding boxes and class probabilities directly for each cell, enabling end-to-end learning (Redmon et al. 2016). YOLOv1 utilized a simplified CNN backbone, setting the stage with basic bounding box predictions. Subsequent versions like YOLOv2 incorporated the DarkNet-19 backbone and refined anchor boxes through K-means clustering. YOLOv3 expanded this further with a DarkNet-53 architecture, integrating multi-scale detection and residual connections. The series continued to innovate with YOLOv4 implementing CSPDarkNet-53 and PANet alongside mosaic data augmentation. YOLOv5 and YOLOv6 introduced CSPNet with dynamic anchor refinement and further enhancements in PANet, respectively. YOLOv7 featured an EfficientRep backbone with dynamic label assignment, while YOLOv8 introduced a Path Aggregation Network with Dynamic Kernel Attention. YOLOv9 developed multi-level auxiliary feature extraction, and YOLOv10 optimized the system with a lightweight classification head and distinct spatial and channel transformations. YOLOv11 introduced the C3k2 block in its backbone and utilized C2PSA for improved spatial attention. The latest, YOLOv12, marks a significant shift towards an attention-centric design, introducing the Area Attention (A2) module for efficient large receptive field processing, Residual Efficient Layer Aggregation Networks (R-ELAN) for enhanced feature aggregation, and architectural optimizations including FlashAttention and adjusted MLP ratios. This attention-based approach allows YOLOv12 to achieve state-of-the-art performance in both accuracy and efficiency, surpassing previous CNN-based models while maintaining real-time detection capabilities (Tian et al. 2025). autonomous vehicles and traffic safety (Gheorghe et al. 2024; Wang et al. 2024; Shoman et al. 2024a, c), healthcare (Patel et al. 2023; Luo et al. 2021; Salinas-Medina and Neme 2023), industrial applications (Pham et al. 2023; Klarák et al. 2024; Wang et al. 2024), surveillance and security (Arroyo et al. 2019; Bordoloi et al. 2020) and agriculture (Badgujar et al. 2024; Li et al. 2022; Fu et al. 2021; Zhong et al. 2018; Wang et al. 2022; Jiang et al. 2022; Chen et al. 2023; Yu et al. 2024; Jia et al. 2023; Umar et al. 2024; Sapkota et al. 2024a), where accuracy and speed are crucial.

In autonomous vehicles and traffic safety, YOLO has been widely adopted to improve safety in both road transport and aviation, allowing real-time object detection for collision avoidance, traffic monitoring, pedestrian detection, traffic sign detection, and aviation hazard analysis. This application aids in minimizing accidents and improving operational efficiency (Gheorghe et al. 2024; Bakirci and Bayraktar 2024; Wang et al. 2024).

In healthcare, YOLO has been instrumental in assisting and improving diagnostic processes and treatment outcomes. The applications include, but are not limited to, cancer detection (Prinzi et al. 2024; Aly et al. 2021), skin segmentation (Ünver and Ayan 2019), and pill identification (Tan et al. 2021; Suksawatchon et al. 2022) which showcase the model’s ability to adapt to different needs, and essential tasks.

In industrial applications, YOLO aids in surface inspection processes to detect defects and anomalies (Pham et al. 2023; Klarák et al. 2024), structural health monitoring (Pratibha et al. 2023), reliability and safety (Fahim and Hasan 2024) ensuring quality control in manufacturing and production (Pham et al. 2023).

Surveillance and security systems also leverage YOLO for real-time monitoring and rapid identification of suspicious activities (Arroyo et al. 2019; Bordoloi et al. 2020). By integrating these models into surveillance systems, security personnel can more effectively monitor and respond to potential threats, enhancing public safety (Gorave et al. 2020). Similarly, in the context of public health measures like social distancing and face mask detection during pandemics (Kolpe et al. 2022; Bashir et al. 2023), YOLO models provided essential support in enforcing health regulations.

In agriculture, YOLO models have been applied to detect and classify crops (Ajayi et al. 2023), pests, and diseases (Morbekar et al. 2020; Li et al. 2022; Cheeti et al. 2021), facilitating precision agriculture techniques and automating farming operations to increase productivity and optimizing inputs. Additionally, in remote sensing, YOLO contributes to object recognition in satellite (Pham et al. 2020; Cheng et al. 2021) and aerial imagery (Chen et al. 2023; Luo et al. 2022), which supports urban planning, land use mapping, and environmental monitoring. These capabilities demonstrate YOLO’s contribution to critical global challenges such as urban development and environmental conservation.

1.5 Motivation and organization of the study
Since “You Only Look Once” has been widely adopted in the field of computer vision, a search for this keyword in Google Scholar yields approximately 5,550,000 results as of June 9, 2024. The acronym “YOLO” further emphasizes its popularity, generating around 210,000 search results at the same time instant. Thousands of researchers have cited YOLO papers, highlighting its significant influence. This study aims to review and critically summarize the YOLO’s decadal progress and its advancements over time, as visually summarized in the mind-map, shown in Fig. 2.


![10462_2025_11253_Fig2_HTML](/uploads/9c9ed419-6901-41d5-a5f2-88c359fbb3b8-10462_2025_11253_Fig2_HTML.webp)
This comprehensive analysis starts with Sect. 2: YOLO trajectory, tracing the evolution from YOLOv1 to YOLOv12. In Sect. 3: Context and distinctions of prior YOLO literature offers insights into the background and unique aspects of earlier studies. Section 4: Review of YOLO versions details the key features and improvements of each version. In Sect. 5: Applications various use cases across different domains are highlighted, demonstrating the versatility of YOLO models. Following this, Sect. 6Challenges, limitations and future directions addresses current issues and potential advancements. Finally, the Conclusion section summarizes the findings of this comprehensive review. Each section is further divided into various sub-subsections to present and discuss specific topic areas relevant to the corresponding sections.

2 The evolution of YOLO: trajectory and variants
Fig. 1 illustrates the development timeline of the YOLO models, beginning with the release of YOLOv1 and progressing to the latest version, YOLO12. This timeline highlights the key advancements and iterations in the YOLO series.

YOLOv1 (Redmon et al. 2016) was introduced in 2016 as a novel approach to object detection, offering good accuracy and computational speed by processing images using a single stage network architecture. The first YOLO version laid the foundation for real-time applications of machine vision systems, setting a new standard for subsequent developments.

YOLOv2, or YOLO9000 (Li and Yang 2018; Nakahara et al. 2018), expanded on the foundation of YOLOv1 by improving the resolution at which the model operated and by expanding the capability to detect over 9000 object categories, thus enhancing its versatility and accuracy. YOLOv2 introduced two primary variants: a smaller version optimized for speed and a larger version focused on higher accuracy.

YOLOv3 further advanced these capabilities by implementing multi-scale predictions and a deeper network architecture, which allowed better detection of smaller objects (Kim et al. 2018). YOLOv3 introduced three primary variants, each designed to balance model size and performance: YOLOv3-spp (Small), Standard, and YOLOv3-tiny(Tiny), catering to different trade-offs between speed and accuracy.

The series continued to evolve with YOLOv4 and YOLOv5, each introducing more refined techniques and optimizations to improve detection performance (i.e., accuracy and speed) even further (Nepal and Eslamiat 2022; Sozzi et al. 2022; Mohod et al. 2022). YOLOv4 introduced four main variants: the standard version, YOLOv4-CSP, which incorporates Cross-Stage Partial (CSP) networks to enhance performance and reduce computational cost; YOLOv4x-mish, which utilizes the Mish activation function to improve accuracy while maintaining efficiency; and YOLOv4-tiny, a lightweight version optimized for real-time applications and edge devices, sacrificing some accuracy for speed. YOLOv5, developed by Ultralytics (2020) Footnote1, brought significant improvements in terms of ease of use and performance, establishing itself as a popular choice in the computer vision community. YOLOv5 introduced five primary variants to meet various performance needs: YOLOv5s (small), optimized for speed and efficiency in resource-constrained environments; YOLOv5m (medium), offering a balanced trade-off between speed and accuracy; YOLOv5l (large), designed for higher accuracy at the expense of resources; YOLOv5x (extra-large), focused on top-tier accuracy for powerful hardware; and YOLOv5n (nano), a lightweight version tailored for rapid inference and low computational demands, ideal for real-time applications and edge devices.

Subsequent versions, YOLOv6 through YOLO11, have continued to build on this success, focusing on enhancing model scalability, reducing computational demands, and improving real-time performance metrics.

Li et al. (2022) introduced YOLOv6 in 2022. Developed by a team from Meituan, a Chinese e-commerce platform, YOLOv6 features a novel backbone and neck architecture. It also incorporates advanced training techniques such as Anchor-Aided Training (AAT) and Self-Distillation to enhance performance and efficiency. YOLOv6 introduces three main variants: the standard version, balancing accuracy and speed for general detection tasks; YOLOv6-Nano, optimized for real-time applications with a focus on speed and performance on edge devices; and YOLOv6-Tiny, designed for even faster inference on low-resource hardware, trading off some accuracy.

YOLOv7 (Wang et al. 2022, 2023) introduces advanced techniques like trainable bag-of-freebies (optimizations that improve accuracy without increasing inference cost) and dynamic label assignment. It introduces three variants: the standard version, balancing speed and accuracy; YOLOv7-X, a more powerful variant optimized for performance but requiring more computational resources; and YOLOv7-Tiny, a lightweight version designed for real-time applications on edge devices, prioritizing speed over accuracy.

YOLOv8 was released in 2023 by Ultralytics (Jocher et al. 2023). It features a more efficient architecture, enhanced training techniques, and support for larger datasets. Its user-friendly implementation in PyTorch makes it accessible for both research and production. YOLOv8 introduces four variants: YOLOv8-S, optimized for fast inference on edge devices with some accuracy trade-offs; YOLOv8-M, balancing accuracy and speed for general tasks; YOLOv8-L, prioritizing accuracy at the cost of computational demand; and YOLOv8-Tiny, a lightweight version for real-time applications.

YOLOv9 (Wang et al. 2024) proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture – Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN’s architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. Its variants are YOLOv9t, YOLOv9s, YOLOv9m, YOLOv9c, YOLOv9e (Ultralytics 2023a).

YOLOv10 (Ultralytics 2023b), developed by researchers at Tsinghua University, introduces a novel approach to real-time object detection, addressing the limitations of both post-processing and model architecture in previous YOLO versions. By eliminating non-maximum suppression (NMS) and optimizing key components of the model, YOLOv10 offers significant improvements in efficiency and performance. This version introduces six distinct variants as YOLOv10-N, YOLOv10-S, YOLOv10-M, YOLOv10-B, YOLOv10-L, and YOLOv10-X (Wang et al. 2024).

Notably, YOLOv10-N and YOLOv10-S exhibit the lowest latencies at 1.84 ms and 2.49 ms, respectively, making them highly suitable for applications requiring low latency. These models outperform their predecessors, with YOLOv10-X achieving the highest mAP of 54.4% and a latency of 10.70 ms, reflecting a well-balanced enhancement in both accuracy and inference speed.

YOLOv11 represents a significant advancement in object detection, featuring a sophisticated backbone and neck architecture for enhanced feature extraction. It optimizes speed and efficiency while maintaining high accuracy. YOLOv11 balances precision and computational efficiency, suitable for various applications from embedded systems to large-scale deployments. The model comes in five variants: YOLOv11n, YOLOv11s, YOLOv11m, YOLOv11L, and YOLOv11x, based on network depth.

YOLOv12 (Tian et al. 2025) further revolutionizes the field with an attention-centric approach. It introduces the Area Attention (A2) module and Residual Efficient Layer Aggregation Networks (R-ELAN) for improved feature processing. Utilizing these architectural changes, YOLOv12 achieves state-of-the-art performance while maintaining real-time detection capabilities. For instance, YOLOv12-N was reported to achieve 40.6% mAP with a 1.64 ms inference latency on a T4 GPU, outperforming YOLOv10-N and YOLOv11-N by 2.1 mAP with a comparable speed. YOLOv12 also demonstrated superior object contour definition and foreground activation compared to its predecessors. It supports various tasks, including object detection, segmentation, classification, pose estimation, and oriented object detection, making it a versatile tool for diverse computer vision applications. YOLOv12 is available in five variants: YOLOv12-N, YOLOv12-S, YOLOv12-M, YOLOv12-L, and YOLOv12-X, each offering different trade-offs between performance and computational requirements (Tian et al. 2025).

To summarize, each iteration of the YOLO series has set new benchmarks for object detection capabilities and significantly impacted various application areas, from autonomous vehicles and traffic safety

2.1 Significance of latency and mAP scores in YOLO
Inference Time (
) and mean Average Precision (mAP) are critical metrics to assess the performance of object detection models such as YOLO (Tang and Yuan 2015; Mao et al. 2019). Inference Time specifically measures the duration required for the model to process an image and generate predictions, focusing solely on the computational phase and is typically measured in milliseconds (ms) (Mao et al. 2019). This metric excludes any delays from image pre-processing or post-processing, providing a clear measure of the model’s computational efficiency. Lower inference times are crucial for real-time applications such as autonomous driving, surveillance, and robotics, where rapid and accurate detections are essential (Chen et al. 2020). High inference times can lead to delays that compromise safety and effectiveness in these dynamic settings (Pestana et al. 2021).

Frames Per Second (FPS) is another essential metric that indicates how many images the model can evaluate each second, complementing inference time by illustrating the model’s ability to handle streaming video or rapid image sequences. Both inference time and FPS provide a detailed view of the real-time operational performance of a model.

It is also important to note that these performance metrics are highly dependent on the hardware platform used for testing. Differences in computational power can significantly influence results, which makes it essential to standardize hardware during benchmark tests to ensure fair comparisons. Likewise, mAP is a comprehensive metric used to evaluate the accuracy of object detection models (Zhou et al. 2018). It considers both precision and recall (Table 1), and it is calculated by taking the average precision (AP) across all classes and then averaging these AP scores (Hall et al. 2020; Zhou et al. 2018). It provides a balanced view of how well the model performs across different object categories and varying conditions within the dataset. Other metrics used for comprehensive evaluation of YOLO models (Goutte and Gaussier 2005; Liang et al. 2021) are detailed in Table 1.


![Screenshot 2025-11-28 160825](/uploads/876bf58b-6ad6-45ca-b805-a15d1fbcdaae-Screenshot-2025-11-28-160825.png)

Here, True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN) are the key performance evaluators. TP is instance where the model correctly identifies an object as present. TN occurs when the model correctly predicts the absence of an object. FP arises when the model incorrectly identifies an object as present, and FN happens when the model fails to detect an object that is actually present. These metrics are crucial for assessing the accuracy and reliability of the YOLO object detection (Hall et al. 2020; Zhou et al. 2018; Liang et al. 2021).


![10462_2025_11253_Fig3_HTML](/uploads/77c3f210-6543-4dcb-95fa-144946c16374-10462_2025_11253_Fig3_HTML.webp)

The Single Shot MultiBox Detector (SSD) (Liu et al. 2016) (Fig. 3) introduced in 2016 revolutionized object detection by streamlining the process through a single-stage approach, significantly inspiring subsequent developments in YOLO models (Liu et al. 2016; Fu et al. 2017; Zhang et al. 2018). Unlike two-stage models like R-CNN, which rely on a region proposal step before actual object detection, SSD and by extension, YOLO variants, perform detection and classification in a single sweep across the image. This paradigm shift enhances the detection process by eliminating intermediate steps, thus facilitating faster and more efficient object detection suitable for real-time applications. The architecture of SSD, which YOLO models have adapted, utilizes multiple feature maps at different resolutions to detect objects of various sizes, employing a diverse array of anchor boxes at each feature map location to improve localization accuracy (Cui et al. 2018; Lin et al. 2017).

Figure 3 illustrates a YOLO model that incorporates SSD’s architectural principles to enhance real-time detection capabilities through improved feature extraction using Multi-Headed Attention layers. This adoption from SSD methodology significantly boosts the processing speed and detection accuracy of models such as YOLOv8, YOLOv9, and YOLOv10, making them ideal for rapid and reliable object detection in resource-constrained environments (Tang et al. 2018; Li et al. 2017). The efficient single-shot mechanism, which directly classifies and localizes objects, highlights the ongoing evolution of the YOLO series to meet the accuracy and speed requirements of diverse real-world scenarios (Zhang et al. 2018).

3 Prior YOLO literature: context and distinctions
We collected the existing published literature on YOLO to document and critically analyze the past knowledge, including major highlights and limitations, which are briefly summarized and discussed here:

A review of YOLO algorithm developments by Jiang et al. (2022) provided an insightful overview of YOLO algorithm development and its evolution through its versions. The authors analyze the fundamental aspects of YOLO’s to object detection, comparing its various iterations to traditional CNNs. They emphasize ongoing improvements in YOLO, particularly in enhancing target recognition and feature extraction capabilities. It also discusses the application of YOLO in specific fields, such as finance, highlighting its practical implications in feature extraction for image-based news analysis (Jiang et al. 2022).

A comprehensive systematic review of YOLO for medical object detection (2018 to 2023) by Ragab et al. (2024) presented a systematic review of YOLO’s application in the medical field, that analyzes how different variants, particularly YOLOv7 and YOLOv8, have been employed for various medical detection tasks. They highlight the algorithm’s significant performance in lesion detection, skin lesion classification, and other critical areas, demonstrating YOLO’s superiority over traditional methods in accuracy and computational efficiency. Despite its successes, the review identifies challenges, such as the need for well-annotated datasets and addresses the high computational demands of YOLO implementations. The paper suggested directions for future research to optimize YOLO’s application in medical object detection (Ragab et al. 2024).

A comprehensive review of YOLO architectures in computer vision: from YOLOv1 to YOLOv8 and YOLO-NAS by Terven et al. (2023) provides an extensive analysis of the evolutionary trajectory of the YOLO algorithm, detailing how each iteration has contributed to advances in real-time object detection. Their review covers the significant architectural and training enhancements from YOLOv1 through YOLOv8 and introduces YOLO-NAS and YOLO with Transformers. This study serves as a valuable resource for understanding the progression in network architecture, which has progressively improved YOLO’s efficacy in diverse applications such as robotics and autonomous driving.

YOLOv1 to v8: unveiling each variant-a comprehensive review of YOLO by Hussain (2024), provided in-depth analyses of the internal components and architectural innovations of each YOLO variant. It provided a deep dive into the structural details and incremental improvements that have marked the evolution of YOLO, presenting a well-structured analysis complete with performance benchmarks. This methodological approach not only highlights the capabilities of each variant but also discusses their practical impact across different domains, suggesting the potential for future enhancements like federated learning to improve privacy and model generalization (Hussain 2024).

YOLO-v1 to YOLO-v8, the rise of YOLO and its complementary nature toward digital manufacturing and industrial defect detection by Hussain (2023) reviewed and showed rapid progression of the YOLO variants, focusing on their critical role in industrial applications, specifically for defect detection in manufacturing. Starting with YOLOv1 and extending through YOLOv8, the paper illustrates how each version has been optimized to meet the demanding needs of real-time, high-accuracy defect detection on constrained devices. Hussain’s work not only examines the technical advancements within each YOLO iteration but also validates their practical efficacy through deployment scenarios in the manufacturing sector, emphasizing YOLO’s alignment with industrial needs (Hussain 2023).

YOLOv1 to YOLOv10: The fastest and most accurate real-time object detection systems by Wang et al. (2024) provide a comprehensive literature survey on the YOLO series, from YOLOv1 to YOLOv10. This review uniquely revisits the characteristics of YOLO through a contemporary technical lens, highlighting its ongoing influence in advancing real-time computer vision research and subsequent technological developments. The authors explore the evolution of YOLO’s methodologies over the past decade and its diverse applications in fields requiring real-time object analysis. By doing so, they underscore YOLO’s role as a foundational technology in various computer vision tasks, such as instance segmentation and 3D object detection.

Evaluating the evolution of YOLO models: a comprehensive benchmark study of YOLO11 and its predecessors by Jegham et al. (2024) conducts a detailed benchmark analysis of YOLO models from YOLOv3 to YOLO11. It examines their performance across three diverse datasets: Traffic Signs, African Wildlife, and Ships and Vessels, focusing on different challenges like object size and aspect ratio. Employing metrics like Precision, Recall, mAP, Processing Time, GFLOPs, and Model Size, the study identifies the strengths and limitations of each version, with YOLO11m showing exceptional balance in accuracy and efficiency across the datasets.

For the comprehensive review articles, it is advantageous to pinpoint a specific gap that the proposed review will address. For instance, a common oversight in the existing literature is the omission of the latest YOLO iterations, particularly YOLOv9, YOLOv10, YOLOv11 and YOLOv12 or neglecting to cover the application domains of interest. Given the YOLO algorithm’s ten-year milestone, there is a pressing need to systematically document and critically evaluate these newer models. Our review aims to fill this void by providing updated, in-depth insights and comparative analysis of YOLOv9 and YOLOv12, extending across various applications to serve the wider research and technical community. This state-of-the-art review intends to highlight the continued advancements and capabilities of these models within the dynamic field of object detection technology.

In this review paper, we adopt a unique reverse-chronological approach to analyze the progression of YOLO, beginning with the most recent versions and moving backwards. The analysis is divided into six distinct subsections. The first subsection covers the latest iterations, YOLOv12 and YOLOv11, The second subsection examines YOLOv10, YOLOv9, and YOLOv8, where we delve into the architecture and advancements that define the forefront of object detection technology. This approach not only shows the most cutting-edge developments but also sets the stage for understanding the incremental improvements that have been realized over time. The third subsection reviews YOLOv7, YOLOv6, and YOLOv5, tracing further back in the series to highlight the evolutionary steps contributing to the enhancements observed in the later versions. We analyze each model’s technical and scientific aspects to provide a comprehensive view of the progress within these iterations. The fourth subsection addresses the earlier YOLO versions, offering a complete historical perspective that enriches the reader’s understanding of the foundational technologies and the methodologies, refined through successive updates. The fifth subsection presents alternative versions derived from YOLO.

To close this section, we discuss the application of the YOLO models in reverse order across five critical real-world domains: autonomous vehicles and traffic safety, healthcare and medical image analysis, surveillance and security, industrial manufacturing and agriculture. For each application, we present a detailed examination and corresponding tabular data in reverse chronological order, showcasing how YOLO technologies have been adapted and implemented to meet specific industry needs and challenges. This reverse review strategy not only emphasizes the state-of-the-art but also provides a narrative of technological evolution, illustrating how each iteration builds upon the last to push the boundaries of what’s possible in object detection. By understanding where YOLO technology stands today and how it got there, readers gain a comprehensive view of its capabilities and potential future directions. This methodical unpacking of the YOLO series not only highlights technological advancements but also offers insights into the broader implications and utility of these models in practical scenarios, setting the groundwork for anticipating future innovations in object detection technology.

4 Review of YOLO versions
This section reviews YOLO series models, starting from the advanced and latest version, YOLOv12, and progressively tracing back to the foundational YOLOv1. By first highlighting the most recent technological advancements, this approach enables immediate insights into the state-of-the-art capabilities of object detection. Subsequently, the narrative explores how earlier models laid the groundwork for these innovations.

4.1 YOLOv12 and YOLO11
YOLOv12 (Tian et al. 2025) is the most recent YOLO version introduced in February 2025, which marks a substantial advancement in real-time object detection by integrating attention mechanisms into the YOLO framework while maintaining competitive inference speeds. This attention-centric framework not only surpasses popular real-time detectors (e.g., Faster R-CNN, RetinaNet, Detectron 2) in accuracy but also achieves state-of-the-art performance through a combination of innovative attention methods, Residual Efficient Layer Aggregation Networks (R-ELAN), and several architectural optimizations.


![10462_2025_11253_Fig4_HTML](/uploads/a4109bc5-f65c-41fd-908e-9844463bf032-10462_2025_11253_Fig4_HTML.webp)

Fig. 4a demonstrates latency comparisons on the MS COCO benchmark dataset, highlighting YOLOv12’s significantly lower inference latency compared to YOLOv11, YOLOv10, YOLOv9, and YOLOv8. The curve reveals a substantial reduction in latency, enabling faster processing speeds while maintaining high detection accuracy (Tian et al. 2025). Complementing this, Fig. 4b presents comparison in terms of GFLOPs, which shows YOLOv12 achieves higher computational efficiency, reflecting its ability to handle complex computations effectively. This balance between speed and computational power demonstrates YOLOv12’s robust performance.

On the COCO dataset, YOLOv12 sets a new state-of-the-art standard, with the lightweight YOLOv12-N achieving 40.6% mAP and the larger YOLOv12-X reaching 55.2% mAP. These results, combined with the latency and GFLOPs improvements, establish YOLOv12 as a new benchmark in real-time object detection. The model framework is available in five scales: YOLOv12-N, S, M, L, and X, each optimized for specific applications. For instance, YOLOv12-N achieves 40.6% mAP at 1.64 ms on a T4 GPU, outperforming YOLOv10-N and YOLOv11-N by 2.1% and 1.2% mAP, respectively. Similarly, YOLOv12-S attains 48.0% mAP at 2.61 ms/image, surpassing YOLOv8-S, YOLOv9-S, YOLOv10-S, and YOLOv11-S by margins of 3.0%, 1.2%, 1.7%, and 1.1% mAP. The larger models in the YOLOv12 family continue to show improvements in efficiency and performance. Notably, YOLOv12-M achieves 52.5% mAP at 4.86 ms/image. In terms of computational effieciency, YOLOv12-L demonstrates a significant reduction in FLOPs, decreasing by 31.4G compared to its predecessor, YOLOv10-L. At the highest end of the scale, YOLOv12-X showcases superior performance, outperforming both YOLOv10-X and YOLOv11-X in detection accuracy (Tian et al. 2025).

YOLOv12 also surpasses end-to-end detectors like RT-DETR and RT-DETRv2. For example, YOLOv12-S runs 42% faster than RT-DETR-R18 and RT-DETRv2-R18, using only 36% of the computation and 45% of the parameters. Residual connections show minimal impact on convergence in smaller models (YOLOv12-N) but are critical for stable training in larger models (YOLOv12-L/X), with YOLOv12-X requiring a scaling factor of 0.01. The area attention module reduces inference time by 0.7 ms on an RTX 3080 with FP32 precision, while FlashAttention further accelerates inference by 0.3-
0.4 ms.

Visualization analyses confirm that YOLOv12 produces clearer object contours and more precise foreground activations than its predecessors. A convolution-based attention implementation proves to be faster than linear alternatives. Additionally, a hierarchical design, extended training (approximately 600 epochs), an optimized convolution kernel size (
), absence of positional embedding, and an MLP ratio of 1.2 collectively enhance the framework’s performance and efficiency.


![10462_2025_11253_Fig5_HTML](/uploads/0727e5ae-03e3-4b01-8230-aaa43f06a750-10462_2025_11253_Fig5_HTML.webp)


As discussed before, the YOLOv12 architecture (Fig. 5a), demonstrates an advanced integration of A2 (Area Attention) modules, R-ELAN (Residual Efficient Layer Aggregation Networks) blocks, and a streamlined detection head. This design optimizes the model’s visual information processing while maintaining high accuracy. The major innovations on the YOLOv12 architecture are listed below.

4.1.1 YOLOv12 architectural innovation
Area attention (A2) module: This module implements segmented feature processing with Flash Attention integration, reducing computational complexity by 50% through spatial reshaping while maintaining large receptive fields. AA enables real-time detection at fixed 
 resolution through optimized memory access patterns, as illustrated in Fig. 5a.

Residual ELAN (R-ELAN) hierarchy: R-ELAN combines residual shortcuts (0.01 scaling) with dual-branch processing to mitigate the gradient vanishing problem. The model also features a streamlined final aggregation stage that reduces parameters by 18% and FLOPs by 24% compared to baseline architectures, as shown in Fig. 5b.

Efficient architectural revisions: YOLOv12 replaces positional encoding with 7
7 depth-wise convolution for implicit spatial awareness. It also implements adaptive MLP ratio (1.2
) and shallow block stacking to balance the computational load, achieving 4.1 ms inference latency on V100 hardware.

Optimized training framework: The model was trained over 600 epochs using SGD with cosine scheduling (initial lr = 0.01). The model also incorporates Mosaic-9 and Mixup augmentations with 12.8% mAP gain on COCO dataset, maintaining real-time performance through selective kernel convolution integration.

Figure 5b presents an architectural comparison of popular attention modules: CSPNet, ELAN, C3K2 (a case of GELAN), and the proposed R-ELAN. Brief summary of these modules is blow.

CSPNet (Cross stage partial network): CSPNet enhances gradient flow by splitting feature maps into two paths, one for learning and one for propagation, reducing computational bottlenecks and improving inference speed. This model is visually depicted in Fig. 5b (leftmost module).

ELAN (Efficient layer aggregation network): ELAN improves feature integration by aggregating multi-scale features efficiently, enhancing the model’s ability to detect objects at various scales. However, as shown in Fig. 5b (second module), ELAN can introduce instability due to gradient blocking and lacks of residual connections, particularly in large-scale models.

C3K2 (Compact GELAN): This module is a compact version of GELAN (Generalized Efficient Layer Aggregation Network) that offers a balance between computational efficiency and feature expressiveness, suitable for resource-constrained environments. The module is also illustrated in Fig. 5b (third module).

R-ELAN (Residual ELAN): R-ELAN introduces residual connections and redesigns feature aggregation to address optimization challenges in attention-based models, combining the benefits of residual learning with efficient feature aggregation. As shown in Fig. 5b (rightmost module), R-ELAN applies a residual shortcut with a scaling factor (default 0.01) and processes the input through a transition layer, followed by a bottleneck structure for improved stability and performance.

The R-ELAN design, as depicted in Fig. 5b, addresses the limitations of ELAN by introducing residual connections and a revised aggregation approach. Unlike ELAN, which splits the input into two parts and processes them separately, R-ELAN applies a transition layer to adjust channel dimensions and processes the feature map through subsequent blocks before concatenation. This design mitigates gradient blocking and ensures stable convergence, particularly in large-scale models like YOLOv12-L and YOLOv12-X. The integration of residual connections and attention mechanisms in R-ELAN, as shown in Fig. 5b, highlights YOLOv12’s architectural advancements in balancing efficiency and accuracy.

YOLO11: YOLO11 developed by Ultralytics represents the most recent version building upon the foundations established by its predecessors in the YOLO family. This latest iteration introduces several architectural innovations that enhance its performance across a wide spectrum of tasks as depicted in Fig. 6. The model incorporates the C3k2 (Cross Stage Partial with kernel size 2) block, which replaces the C2f block used in previous versions, offering improved computational efficiency (Sapkota et al. 2024b). Additionally, YOLOv11 retains the SPPF (Spatial Pyramid Pooling-Fast) component and introduces the C2PSA (Convolutional block with Parallel Spatial Attention) module, collectively enhancing feature extraction capabilities (Sapkota et al. 2024a). These architectural enhancements enable YOLOv11 to capture intricate image details with greater precision, particularly in challenging scenarios involving small or occluded objects. The model’s versatility is evident in its support for a broad range of computer vision tasks, including object detection, instance segmentation, pose estimation, image classification, and oriented bounding box (OBB) detection (Ultralytics 2024).

Empirical evaluations of YOLOv achieves a higher mean Average Precision (mAP) score on the COCO dataset while utilizing 22% fewer parameters compared to its YOLOv8m counterpart (Jegham et al. 2024). This reduction in parameter count contributes to faster model performance without significantly impacting overall accuracy. Furthermore, YOLOv11 exhibits inference times approximately 2% faster than YOLOv10, making it particularly well-suited for real-time applications. The model’s efficiency extends across various deployment environments, including edge devices, cloud platforms, and systems supporting NVIDIA GPUs. YOLOv11 is available in multiple variants, ranging from nano to extra-large, catering to diverse computational requirements and use cases. These advancements position YOLOv11 as a state-of-the-art solution for industries requiring rapid and accurate image analysis, such as autonomous driving, surveillance, and industrial automation.


![10462_2025_11253_Fig6_HTML](/uploads/aaff0ae2-4b74-43b0-b5b9-27715fd5a0e9-10462_2025_11253_Fig6_HTML.webp)



![10462_2025_11253_Fig7_HTML](/uploads/b235e2b2-5107-4de0-bc2c-f78d4389beac-10462_2025_11253_Fig7_HTML.webp)

YOLOv10, incorporates advanced techniques like automated architecture search and more refined loss functions to enhance detection accuracy and speed, tailored for both edge and cloud computing environments. This version and its predecessors, YOLOv9 and YOLOv8, introduce substantial improvements in network architecture, such as the integration of cross-stage partial networks (CSPNets) and the use of transformer-based backbones for better feature extraction across different scales. YOLOv7 and YOLOv6 continued to build on these improvements by optimizing computational efficiency and expanding model scalability. Meanwhile, YOLOv5 introduced PyTorch support, which significantly enhanced the model’s accessibility and adaptability, thus broadening its application in industry and academia. YOLOv4, on the other hand, marked a pivotal point in YOLO history by integrating features like Mish activation and Cross-Stage Partial connections, setting new standards for speed and accuracy in real-time applications. The mid-generations, starting with YOLOv3, were notable for introducing multi-scale predictions and bounding box predictions across different layers, which greatly improved the model’s ability to detect small objects-a longstanding challenge in earlier versions. YOLOv3 was also one of the first YOLO models to leverage deeper feature extractors like Darknet-53, which significantly boosted its performance over YOLOv2. YOLOv2 itself had introduced important features such as batch normalization and high-resolution classifiers, which enhanced the overall accuracy without compromising the speed. The original YOLO model, YOLOv1, was revolutionary, proposing a single-stage detection framework that unified the object detection process into a single neural network model.

Fig. 7a illustrates a sophisticated transformer-based model that simplifies the detection process by integrating dual label assignments and eliminating the need for non-max suppression (NMS), achieving a streamlined, end-to-end object detection. YOLOv9, as shown in Fig. 7b, introduces the Programmable Gradient Information (PGI) system to enhance model interpretability and robustness, significantly improving generalization across various tasks. Moving to YOLOv8 and YOLOv7, Figs. 7c and d respectively depict their architectures which incorporate elements like ELAN and CSPNet to boost performance and flexibility across computing devices. YOLOv6, highlighted in Fig. 7e, focuses on industry applications with enhancements in model quantization and real-time performance.

YOLOv5, represented in Fig. 7f, marks a pivotal development with its adoption of PyTorch and improvements in training methods that enhance model accessibility and efficiency. In contrast, YOLOv4, shown in Fig. 7g, integrates technologies like CSPNet and Path Aggregation Network (PAN) (Liu et al. 2018) to optimize real-time detection. YOLOv3, visualized in Fig. 7h, introduces significant architectural changes with Darknet-53 and multi-scale predictions, which substantially enhance the detection of small objects. YOLOv2, depicted in Fig. 7i, advances the architecture with dimension clusters and fine-grained features, improving the model’s efficiency and adaptability. Finally, YOLOv1, as outlined in Fig. 7j, revolutionizes object detection by integrating a single-stage detector that performs grid-based predictions in real-time, significantly reducing model complexity and enhancing speed.

4.2 YOLOv10, YOLOv9 and YOLOv8
YOLOv10 (Wang et al. 2024), developed at Tsinghua University, China, represents a breakthrough in the YOLO series for real-time object detection, achieving unprecedented performance. This version eliminates the need for non-maximum suppression (NMS) (Rothe et al. 2015), a traditional bottleneck in earlier models, thereby drastically reducing latency. YOLOv10 introduces a dual assignment strategy in its training protocol, which optimizes detection accuracy without sacrificing speed with the help of one-to-many and one-to-one label assignments, ensuring robust detection with lower latency (Li et al. 2023; Tian et al. 2024). The architecture of YOLOv10 includes several innovative components that enhance both computational efficiency and detection performance. Among these are lightweight classification heads (Bhagat et al. 2021) that reduce computational demands, spatial-channel decoupled downsampling to minimize information loss during feature reduction (Hu et al. 2023), and rank-guided block design that optimizes parameter use (Yang et al. 2023). These architectural advancements ensure that YOLOv10 operates synergistically across various scales-from YOLOv10-N (Nano) to YOLOv10-X (Extra Large), making it adaptable to diverse computational constraints and operational requirements (Wang et al. 2024). According to wang et al. (Wang et al. 2024), performance evaluations on benchmark datasets like MS-COCO (Lin et al. 2014) demonstrate that YOLOv10 not only surpasses its predecessors-YOLOv9 and YOLOv8-in both accuracy and efficiency but also sets new industry standards. For instance, YOLOv10-S substantially outperforms comparable models (e.g., YOLOv9 BASE, YOLOV9 Gelan, YOLOv8, YOLOv7) with an improved mAP and lower latency. This version also incorporates holistic efficiency-accuracy driven design, large-kernel convolutions, and partial self-attention modules, collectively improving the trade-off between computational cost and detection capability. The architecture diagrams of YOLOv10, YOLOv9, and YOLOv8 are summarized in Figs. 8, 9, and 10, respectively.


![10462_2025_11253_Fig8_HTML](/uploads/c137503e-d3c0-4cb3-901a-6c0e2b28f825-10462_2025_11253_Fig8_HTML.webp)

The YOLOv10 model offers various configurations, each tailored to specific performance needs within real-time object detection frameworks. Starting with YOLOv10-N (Nano), it demonstrates a rapid detection capability with a mAP of 38.5% at an exceptionally reduced latency to 1.84 ms, making it highly suitable for scenarios demanding quick responses. Progressing through the series, YOLOv10-S (Small) and YOLOv10-M (Medium) offer progressively higher mAP values of 46.3% and 51.1% at latencies of 2.49 ms and 4.74 ms, respectively, providing a balanced performance for versatile applications. The larger variants, YOLOv10-B (Balanced) and YOLOv10-L (Large), cater to environments requiring detailed detections, with mAPs of 52.5% and 53.2% and latencies of 5.74 ms and 7.28 ms respectively. The largest model, YOLOv10-X (Extra Large), excels with the highest mAP of 54.4% at a latency of 10.70 ms, designed for complex detection tasks where precision is paramount. These configurations underscore YOLOv10’s adaptability across a spectrum of operational requirements.

Reflecting on YOLO’s evolution, starting from YOLOv1, which set the benchmark with an mAP of 63.4% and a latency of 45 ms, to the latest YOLOv10, significant technological strides have been evident. YOLOv10’s predecessors, YOLOv9 and YOLOv8, display comparable mAP scores to YOLOv10 but with marginally higher latency, indicating the incremental enhancements YOLOv10 brings to the table. Specifically, YOLOv9 and YOLOv8 models, such as YOLOv9-N and YOLOv8-N, showcase mAPs of 39.5% and 37.3%, respectively, at latency indicative of their generational improvements. Meanwhile, the higher end of these series, YOLOv9-X, and YOLOv8-X, achieve mAPs of 54.4% and 53.9%, respectively, with YOLOv10 outperforming them in efficiency. The YOLO series, from YOLOv1 through YOLOv8, YOLOv9, and now YOLOv10, has continually advanced the frontier of real-time object detection, enhancing both the speed and accuracy of detections, and thus broadening the scope for practical applications in sectors like autonomous driving, surveillance, and real-time video analytics.

YOLOv9 (Wang et al. 2024) marks a significant advancement in real-time object detection by addressing the efficiency and accuracy challenges associated with earlier versions, particularly by mitigating information loss in deep neural processing. It introduces the innovative Programmable Gradient Information (PGI) and the Generalized Efficient Layer Aggregation Network (GELAN) architecture. These enhancements focus on preserving crucial information across the network, ensuring robust and reliable gradients that prevent data degradation, which is common in deep neural networks (Tishby and Zaslavsky 2015). Compared to its successor, YOLOv10, YOLOv9 sets a foundational stage by addressing the information bottleneck problem that typically hinders deep learning models. While YOLOv9’s PGI strategically maintains data integrity throughout the processing layers, YOLOv10 builds upon this foundation by eliminating the need for NMS and further optimizing model architecture for reduced latency and enhanced computational efficiency. YOLOv10 also introduces dual assignment strategies for NMS-free training, significantly enhancing the system’s response time without compromising accuracy, which reflects a direct evolution from the groundwork laid by YOLOv9’s innovations (Zhang et al. 2023). Furthermore, YOLOv9’s GELAN architecture represents a pivotal improvement in network design, offering a flexible and efficient structure that effectively integrates multi-scale features. While GELAN contributes significantly to YOLOv9’s performance, YOLOv10 extends these architectural improvements to achieve even greater efficiency and adaptability (Chien et al. 2024). It reduces computational overhead and increases the model’s applicability to various real-time scenarios, showcasing an advanced level of refinement that leverages and enhances the capabilities introduced by YOLOv9.

YOLOv8 was released in January 2023 by Ultralytics, marking a significant progression in the YOLO series with an introduction of multiple scaled versions designed to cater to a wide range of applications (Ultralytics 2024a, b). These versions included YOLOv8n (nano), YOLOv8s (small), YOLOv8m (medium), YOLOv8l (large), and YOLOv8x (extra-large), each optimized for specific performance and computational needs. This flexibility made YOLOv8 highly versatile, supporting many vision tasks such as object detection, segmentation, pose estimation, tracking, and classification, significantly broadening its application scope in real-world scenarios (Ultralytics 2024b). The architecture of YOLOv8 underwent substantial refinements to enhance its detection capabilities. It retained a similar backbone to YOLOv5 but introduced modifications in the CSP Layer, now evolved into the C2f module-a cross-stage partial bottleneck with dual convolutions that effectively combine high-level features with contextual information to bolster detection accuracy. YOLOv8 transitioned to an anchor-free model with a decoupled head, allowing independent processing of object detection, classification, and regression tasks, which, in turn, improved overall model accuracy (Ultralytics 2024). The output layer employed a sigmoid activation function for objectness scores and softmax for class probabilities, enhancing the precision of bounding box predictions. YOLOv8 also integrated advanced loss functions like CIoU (Du et al. 2021) and Distribution Focal Loss (DFL) (Xu et al. 2022) for bounding-box optimization and binary cross-entropy for classification, which proved particularly effective in enhancing detection performance for smaller objects. YOLOv8’s architecture, demonstrated in detailed diagrams, features the modified CSPDarknet53 backbone with the innovative C2f module, augmented by a spatial pyramid pooling fast (SPPF) layer that accelerates computation by pooling features into a fixed-size map. This model also introduced a semantic segmentation variant, YOLOv8-Seg, which utilized the backbone and C2f module, followed by two segmentation heads designed to predict semantic segmentation masks efficiently. This segmentation model achieved state-of-the-art results on various benchmarks while maintaining high speed and accuracy, evident in its performance on the MS COCO dataset where YOLOv8x reached an AP of 53.9% at 640 pixels image size-surpassing the 50.7% AP of YOLOv5-with a remarkable speed of 280 FPS on an NVIDIA A100 using TensorRT. As we progress backwards through the YOLO series, from YOLOv10 to YOLOv8 and soon to YOLOv7, these architectural and functional advancements highlight the series’ evolutionary trajectory in optimizing real-time object detection networks.

YOLOv8 was released in January 2023 by Ultralytics, marking a significant progression in the YOLO series with an introduction of multiple scaled versions designed to cater to a wide range of applications (Ultralytics 2024a, b). These versions included YOLOv8n (nano), YOLOv8s (small), YOLOv8m (medium), YOLOv8l (large), and YOLOv8x (extra-large), each optimized for specific performance and computational needs. This flexibility made YOLOv8 highly versatile, supporting many vision tasks such as object detection, segmentation, pose estimation, tracking, and classification, significantly broadening its application scope in real-world scenarios (Ultralytics 2024b). YOLOv8’s architecture underwent significant upgrades to boost its detection performance, maintaining a backbone similar to YOLOv5 but enhancing it with the evolved C2f module, a cross-stage partial bottleneck with dual convolutions. This module integrates high-level features with contextual information, improving accuracy. The model transitioned to an anchor-free system with a decoupled head for independent objectness, classification, and regression tasks, enhancing accuracy (Ultralytics 2024). The output layer now uses sigmoid for objectness and softmax for class probabilities, refining bounding box precision. Additionally, YOLOv8 employs CIoU (Du et al. 2021), Distribution Focal Loss (DFL) (Xu et al. 2022) for bounding-box optimization, and binary cross-entropy for classification, significantly boosting performance, particularly for smaller objects.

This model also introduced a semantic segmentation variant, YOLOv8-Seg (Yue et al. 2023), which utilized the backbone and C2f module, followed by two segmentation heads designed to predict semantic segmentation masks efficiently. This segmentation model achieved state-of-the-art results on various benchmarks while maintaining high speed and accuracy, evident in its performance on the MS COCO dataset where YOLOv8x reached an AP of 53.9% at 640 pixels image size-surpassing the 50.7% AP of YOLOv5-with a remarkable speed of 280 FPS on an NVIDIA A100 using TensorRT. As we progress backwards through the YOLO series, from YOLOv10 to YOLOv8 and soon to YOLOv7, these architectural and functional advancements highlight the series’ evolutionary trajectory in optimizing real-time object detection networks.

4.3 YOLOv7, YOLOv6 and YOLOv5
The YOLOv7 model introduces enhancements in object detection tailored for drone-captured scenarios, particularly through the Transformer Prediction Head (TPH-YOLOv5) variant (Zhu et al. 2021), which emphasizes improvements in handling scale variations and densely packed objects (Wang et al. 2023). By incorporating TPH and the Convolutional Block Attention Module (CBAM) (Woo et al. 2018), YOLOv7 substantially boosts its capacity to focus on relevant regions in cluttered environments. These features particularly enhance the model’s ability to detect objects across varied scales, an essential trait for drone applications where altitude changes affect object size perception drastically. The model integrates sophisticated strategies like multi-scale testing (Hnewa and Radha 2023) and a self-trained classifier, which refines its performance on challenging categories by specifically addressing common issues in drone imagery, such as motion blur and occlusion. These adaptations have shown notable improvements, with YOLOv7 achieving competitive results in drone-specific datasets and challenges (Bai et al. 2024). The model’s adaptability and robustness in such specialized conditions demonstrate its potential beyond conventional settings, catering effectively to next-generation applications like urban surveillance and wildlife monitoring.

Fig. 11
figure 11
Architecture diagrams for a YOLOv7; b YOLOv6; and c YOLOv5

Full size image
YOLOv6 emerges as a robust solution in industrial applications by delivering a finely balanced trade-off between speed and accuracy, crucial for deployment across various hardware platforms (Li et al. 2022). It iterates on previous versions by incorporating cutting-edge network designs, training strategies, and quantization techniques to enhance its efficiency and performance significantly. This model has been optimized for diverse operational requirements with its scalable architecture, ranging from YOLOv6-N to YOLOv6-X, each offering different performance levels to suit specific computational budgets (Sirisha et al. 2023). Significant innovations in YOLOv6 include advanced label assignment techniques and loss functions that refine the model’s predictive accuracy and operational efficiency. By leveraging state-of-the-art advancements in machine learning, YOLOv6 not only excels in traditional object detection metrics but also sets new standards in throughput and latency, making it exceptionally suitable for real-time applications in industrial and commercial domains.

YOLOv6 and YOLOv7 each introduced innovative features that build on the foundation set by YOLOv5. YOLOv6, released in October 2021, introduced lightweight nano models optimized for mobile and CPU environments alongside a more effective backbone for improved small object detection. YOLOv7 further advanced this development by incorporating a new backbone network, PANet (Wang et al. 2019), enhancing feature aggregation and representation, and introducing the CIOU loss function for better object scaling and aspect ratio handling. YOLO-v6 significantly shifts the architecture to an anchor-free design, incorporating a self-attention mechanism to better capture long-range dependencies and employing adaptive training techniques to optimize performance during training (Zhang et al. 2021). These versions collectively push the boundaries of object detection performance, emphasizing speed, accuracy, and adaptability across various deployment scenarios.

YOLOv5 has significantly contributed to the YOLO series evolution, focusing on user-friendliness and performance enhancements (Ultralytics 2024a, b). Its introduction by Ultralytics brought a streamlined, accessible framework that lowered the barriers to implementing high-speed object detection across various platforms. YOLOv5’s architecture incorporates a series of optimizations including improved backbone, neck, and head designs which collectively enhance its detection capabilities. The model supports multiple size variants, facilitating a broad range of applications from mobile devices to cloud-based systems (Ultralytics 2024a). YOLOv5’s adaptability is further evidenced by its continuous updates and community-driven enhancements, which ensure it remains at the forefront of object detection technologies. This version stands out for its balance of speed, accuracy, and utility, making it a preferred choice for developers and researchers looking to deploy state-of-the-art detection systems efficiently.

YOLOv5 marks a significant evolution in the YOLO series, focusing on production-ready deployments with streamlined architecture for real-world applications. This version emphasizes reducing the model’s complexity by refining its layers and components, enhancing its inference speed without sacrificing detection accuracy. The backbone and feature extraction layers were optimized to accelerate processing, and the network’s architecture was simplified to facilitate faster data throughput. Importantly, YOLO v5 enhances its deployment flexibility, catering to edge devices with limited computational resources through model modularity and efficient activations. These architectural refinements ensure YOLO v5 operates effectively in diverse environments, from high-resource servers to mobile devices, making it a versatile tool in the arsenal of object detection technologies.

4.4 YOLOv4, YOLOv3, YOLOv2 and YOLOv1
The introduction of YOLOv4 (Bochkovskiy et al. 2020) in 2020 marked the latest developments, employing CSPDarknet-53 (Mahasin and Dewi 2022) as its backbone. This modified version of Darknet-53 uses Cross-Stage Partial connections to reduce computational demands while enhancing learning capacity.

Fig. 12
figure 12
Comparison of YOLOv4 (Bochkovskiy et al. 2020) and YOLOv3 (Redmon and Farhadi 2018) architectures. a YOLOv4 architecture shows a two-stage detector with a backbone, neck, dense prediction, and sparse prediction modules. b YOLOv3 architecture features convolutional and upsampling layers that lead to multi-scale predictions. This highlights the structural advancements in object detection between the two versions

Full size image
YOLOv4 incorporates innovative features such as Mish activation (Misra 2019), replacing traditional ReLU to maintain smooth gradients, and utilizes new data augmentation techniques such as Mosaic and CutMix (Yun et al. 2019). Additionally, it introduces advanced regularization methods, including DropBlock regularization (Ghiasi et al. 2018) and Class Label Smoothing to prevent overfitting (Müller et al. 2019), alongside optimization strategies termed BoF (Bag of Freebies) (Zhang et al. 2019) and BoS (Bag of Specials) that enhance training and inference efficiency. “YOLOv3, introduced in 2018 before the release of YOLOv4, employed the Darknet-53 architecture, incorporating principles of residual learning. Initially trained on ImageNet, this version excelled in detecting objects of various sizes due to its multi-scale detection capabilities within the architecture. The subsequent development of YOLOv4 built upon the success of YOLOv3, further enhancing the framework’s robustness and accuracy.

Fig. 13
figure 13
a YOLOv2 architecture  (Redmon and Farhadi 2017), illustrating improvements such as the use of batch normalization, higher resolution input, and anchor boxes; b YOLOv1 architecture  (Redmon et al. 2016), showing the sequence of convolutional layers, max-pooling layers, and fully connected layers used for object detection. This model performs feature extraction and prediction in a single unified step, aiming for real-time performance

Full size image
YOLOv3 (Redmon and Farhadi 2018) improved detection accuracy, especially for small objects, by using three different scales for detection, thereby capturing essential features at various resolutions. Earlier, YOLOv2 and the original YOLO (YOLOv1) laid the groundwork for these advancements (Redmon et al. 2016).

Earlier, YOLOv2 and the original YOLO (YOLOv1) laid the groundwork for these advancements. Released in 2016, YOLOv2 introduced a new 30-layer architecture with anchor boxes from Faster R-CNN and batch normalization to speed up convergence and enhance model performance. YOLOv1, debuting in 2015 by Joseph Redmon, revolutionized object detection with its single-shot mechanism that predicted bounding boxes and class probabilities in one network pass, utilizing a simpler Darknet-19 architecture. This initial approach significantly accelerated the detection process, establishing the foundational techniques that would be refined in later versions of the YOLO series. YOLOv4 and YOLOv3, showcasing their advanced architectures and features, are illustrated in Fig. 12a and b, respectively, while YOLOv2 and YOLOv1 are depicted in Fig. 13a and b, showcasing the foundational developments in the series.

4.5 Alternative versions derived from YOLO
Several alternative YOLO models have been developed from different versions, with the five primary ones being YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and Gold-YOLO.

4.5.1 YOLO-NAS
YOLO-NAS, developed by Deci AI, represents a significant advancement in object detection technology (Terven et al. 2023). This model leverages Neural Architecture Search (NAS) (Ren et al. 2021) to address limitations of previous YOLO iterations such as YOLOv4, YOLOv5, YOLOv6 and YOLOv7 (Mithun and Jawhar 2024). YOLO-NAS introduces a quantization-friendly basic block, enhancing performance with minimal precision loss post-quantization. The architecture employs quantization-aware blocks and selective quantization, resulting in superior object detection capabilities. Notably, when converted to INT8, the model experiences only a slight precision drop, outperforming its predecessors. YOLO-NAS utilizes sophisticated training schemes and post-training quantization techniques, further improving its efficiency (Terven et al. 2023). The model is pre-trained on datasets such as COCO, Objects365, and Roboflow 100, making it suitable for various downstream object detection tasks. YOLO-NAS is available in three variants: Small (s), Medium (m), and Large (l), each optimized for different computational requirements. These variants offer a balance between Mean Average Precision (mAP) and latency, with the INT-8 versions demonstrating impressive performance metrics. The architecture of YOLO-NAS (Fig. 14a) supports inference, validation, and export modes, though it does not support training. YOLO-NAS’s innovative design and superior performance position it as a critical tool for developers and researchers in the field of computer vision.

Fig. 14
figure 14
Architecture diagram of a YOLONAS; b YOLOX; c YOLOR; d DAMO YOLO; e GOLD YOLO

Full size image
4.5.2 YOLO-X
YOLOX, developed by Megvii Technology, represents a significant advancement in the YOLO series of object detectors. This model introduces several key improvements to enhance performance and efficiency. YOLOX adopts an anchor-free approach, departing from the anchor-based methods of its predecessors such as YOLOv6, YOLOv7 and YOLOv8 (Ge 2021). It incorporates a decoupled head, separating classification and regression tasks to address the known conflict between these objectives in object detection (Zhang et al. 2022). The model also implements SimOTA, an advanced label assignment strategy, further improving its detection capabilities (Liu and Sun 2022). Architecturally (Fig. 14b), YOLOX-DarkNet53 builds upon the YOLOv3-SPP baseline, incorporating enhancements such as EMA weights updating, cosine learning rate scheduling, IoU loss, and an IoU-aware branch. The decoupled head consists of a 1x1 convolution layer for channel dimension reduction, followed by two parallel branches with two 3x3 convolution layers each (Ashraf et al. 2024). This design significantly improves convergence speed and is crucial for end-to-end detection performance. YOLOX demonstrates superior performance across various model sizes. The YOLOX-L variant achieves 50.0% AP on COCO at 68.9 FPS on Tesla V100, surpassing YOLOv5-L by 1.8% AP. Even the lightweight YOLOX-Nano, with only 0.91M parameters and 1.08 GFLOPs, attains 25.3% AP on COCO, outperforming NanoDet by 1.8% AP. These advancements position YOLOX as a state-of-the-art object detector, balancing accuracy and efficiency across a wide range of model scales  (Ge 2021).

4.5.3 YOLO-R
YOLOR (You Only Learn One Representation) is a novel object detection algorithm developed by  Chang et al. (2023). Unlike other YOLO versions, YOLOR introduces a novel approach to multi-task learning by unifying implicit and explicit knowledge representation (Andrei-Alexandru et al. 2022). The algorithm’s core concept is inspired by human cognition, aiming to process multiple tasks simultaneously given a single input. YOLOR’s architecture (Fig. 14c) incorporates three key components: kernel space alignment, prediction refinement, and a CNN with multi-task learning capabilities. This unified network encodes both implicit knowledge (learned subconsciously from deep layers) and explicit knowledge (obtained from shallow layers and clear metadata), resulting in a more refined and generalized representation  (Sun et al. 2024; Chang et al. 2023). Compared to other YOLO algorithms such as YOLOv9, YOLOv5 or YOLOv3, YOLOR significantly improves both speed and accuracy. It achieves comparable object detection accuracy to Scaled YOLOv4 while increasing inference speed by 88%, making it one of the fastest object detection algorithms in modern computer vision. On the MS COCO dataset, YOLOR outperforms PP-YOLOv2 by 3.8% in mean average precision at the same inference speed  (Chang et al. 2023).

4.5.4 DAMO-YOLO
DAMO-YOLO is developed by Alibaba’s DAMO Academy, which significantly enhances performance by integrating novel technologies like Neural Architecture Search (NAS), a reparameterized Generalized-FPN (RepGFPN), and lightweight head architectures (Fig. 14d) with AlignedOTA label assignment and distillation enhancement  (Xu et al. 2022). Leveraging MAE-NAS, the model employs a heuristic, training-free approach to architect detection backbones under strict latency and performance constraints, generating efficient structures akin to ResNet and CSPNet  (Terven et al. 2023). The neck design emphasizes a robust “large neck, small head” architecture, optimizing the fusion of high-level semantic and low-level spatial features through an enhanced FPN. This approach effectively balances computational efficiency and detection accuracy, particularly notable in its deployment across various model scales, from lightweight versions for edge devices to more robust configurations for general industry applications. DAMO-YOLO’s architectural prowess is showcased through impressive performance metrics, achieving mAP scores ranging from 43.6 to 51.9 on COCO datasets with relatively low latency on T4 GPUs. Moreover, the model’s lightweight variants demonstrate substantial efficiency on edge devices, underscoring its adaptability and broad application potential. Such capabilities are further augmented by strategic enhancements in label assignment and knowledge distillation, addressing common challenges in object detection like label misalignment and model generalization.

4.5.5 Gold-YOLO
Gold-YOLO was developed by the team at Huawei Noah’s Ark Lab to significantly enhance multi-scale feature fusion through an innovative Gather-and-Distribute (GD) mechanism  (Wang et al. 2024). This mechanism, which utilizes convolution and self-attention operations, was implemented to optimize the exchange and integration of information across different levels of the feature pyramid. This approach facilitated a more effective balance between latency and detection accuracy  (Wang et al. 2024). Furthermore, an MAE-style unsupervised pretraining was incorporated into the YOLO-series for the first time, which was reported to enhance learning efficiency and overall model performance. Gold-YOLO’s architecture (Fig. 14e) aimed to address the limitations inherent in traditional Feature Pyramid Networks (FPNs) by preventing recursive information loss and enabling more direct and efficient feature fusion. This was achieved by a method where features from all levels were gathered to a central processing node, enhanced, and then redistributed, ensuring enriched feature maps that retained critical information across scales. The effectiveness of this novel design was demonstrated through good performance metrics; Gold-YOLO achieved a 39.9% AP on the COCO dataset with high throughput speeds on a T4 GPU, surpassing previous state-of-the-art models like YOLOv6
3.0-N. The contributions made by this paper were significant, as they not only enhanced the YOLO model’s capabilities to handle various object sizes and complexities but also established a new benchmark for the integration of advanced neural network techniques with traditional convolutional architectures for real-time applications.

5 Applications
YOLO has many real-time practical applications (Vijayakumar and Vairavasundaram 2024; Chen et al. 2023), such as autonomous vehicles and traffic safety where the technology is used for obstacle detection, pedestrian pose estimation for intention prediction, and traffic sign recognition, enhancing safety and navigation (Gheorghe et al. 2024). Similarly, YOLO is employed in healthcare for detecting anomalies in medical images, aiding in accurate and efficient diagnostics (Vijayakumar and Vairavasundaram 2024; Ragab et al. 2024). Additionally, industrial manufacturing benefits from YOLO’s capabilities, with applications in quality control and defect detection (Li et al. 2022; Hussain 2023), in surveillance for intrusion detection and anomaly identification (Mohod et al. 2022), while in agriculture, it supports crop stress detection, monitoring, and robotic fruit harvesting, among other use cases (Alibabaei et al. 2022; Badgujar et al. 2024; Wang et al. 2021).

The remainder of this section is categorically divided into five key application areas where YOLO models have demonstrated significant impact: Autonomous Vehicles and Traffic Safety, Healthcare and Medical Imaging, Security and Surveillance, Industrial Manufacturing, and Agriculture.

5.1 Autonomous vehicles and traffic safety
Each YOLO version has been pivotal in advancing the capabilities of autonomous vehicles and traffic safety by providing highly efficient and accurate real-time detection systems. Each iteration of YOLO has brought improvements that enhance the vehicle’s ability to perceive its environment quickly and accurately, which is critical for safe navigation and decision-making (Benjumea et al. 2021; Malligere Shivanna and Guo 2024). Starting with YOLOv1 (Redmon et al. 2016), the YOLO algorithm revolutionized the approach by performing detection tasks directly from full images in a single network pass, allowing for the detection of objects at a remarkable speed (Sarda et al. 2021). This initial model was pivotal, setting a high standard for real-time object detection and establishing a framework that future versions would build upon. Subsequent iterations, including YOLOv2 and YOLOv3, continued to refine this approach by introducing concepts such as real-time multi-scale processing and improved anchor box adjustments, which enhanced the accuracy and robustness of the detections. These versions were particularly adept at handling the variable scales of objects seen in driving environments-from nearby pedestrians to distant road signs-making them invaluable for autonomous driving applications. YOLOv4 and later versions further pushed the boundaries by integrating advanced neural network techniques and optimizations that improved detection accuracy while maintaining the high-speed processing necessary for real-time applications (Cai et al. 2021; Zhao et al. 2022). These advancements in YOLO technology have not only bolstered the capabilities of autonomous vehicles in terms of environmental perception and decision-making but have also significantly contributed to advancements in automotive safety and operational reliability (Woo et al. 2022).

Ye et al. (2022) developed an end-to-end adaptive neural network control for autonomous vehicles that predicts steering angles using YOLOv5, enhancing vehicle navigation precision (Ye et al. 2022). Mostafa et al. (2022) compared the effectiveness of YOLOv5, YOLOX, and Faster R-CNN in detecting occluded objects for autonomous vehicles, improving detection reliability (Mostafa et al. 2022). Jia et al. (2023) proposed an enhanced YOLOv5 detector for autonomous driving, which offers increased speed and accuracy (Jia et al. 2023). Chen et al. (2023) utilized an improved YOLOv5-OBB algorithm for autonomous parking space detection in electric vehicles, enhancing operational efficiency (Chen et al. 2023). Liu and Yan (2022) customized YOLOv7 for vehicle-related distance estimation, providing essential metrics for safe navigation (Liu and Yan 2022). Mehla et al. (2023) evaluated YOLOv8 against EfficientDet in autonomous maritime vehicles, highlighting the superior detection capabilities of YOLOv8 (Mehla et al. 2023).

Further advancements with YOLOv8 have led to significant improvements in object detection in adverse weather conditions, an area of particular concern for autonomous driving. Applying transfer learning techniques using datasets from diverse weather conditions has markedly increased the detection performance of YOLOv8, ensuring reliable recognition of crucial road elements like pedestrians and obstacles under challenging weather scenarios (Kumar and Muhammad 2023). Additionally, the development of YOLOv8 for specific tasks such as brake light status detection illustrates the algorithm’s flexibility and its potential in enhancing interpretability and safety for autonomous vehicles (Oh and Lim 2023). These innovations underscore the critical role of YOLOv8 and YOLOv9 in pushing the boundaries of what is possible in the autonomous vehicle industry, highlighting their impact in meeting the rigorous demands for safety and reliability in self-driving technologies (Afdhal et al. 2023). YOLOv8 and YOLOv9 are at the forefront of transforming the landscape of autonomous vehicle technologies, playing a pivotal role in enhancing the operational safety and efficiency of self-driving cars. These models have excelled in real-time object detection, a crucial aspect of autonomous driving, especially under the challenging and variable conditions typical in real-world traffic environments. An enhanced version, addresses the need for detecting smaller objects such as traffic signs and signals, demonstrating its utility with a notable accuracy rate and efficiency in processing, making it ideal for high-speed driving scenarios (Wang et al. 2024). Table 2 illustrates different applications of YOLO in the autonomous vehicle industry, presented in reverse chronological order from the most recent versions to the older ones.

Table 2 Studies on YOLO applications focus on object detection and real-time performance improvements to enhance autonomous vehicles and traffic safety
Full size table
5.1.1 Pedestrian pose estimation
Ali et al. (2023) presented a Bayesian Generalized Extreme Value Model to evaluate real-time pedestrian crash risks at signalized intersections, leveraging advanced AI-based video analytics. This framework employs deep learning algorithms like YOLO for precise object detection and DeepSORT for effective tracking. The model concentrates on crucial safety indicators such as Post Encroachment Time (PET). Through this approach, the study underscores the significant role of AI-driven video analysis in boosting intersection safety by delivering real-time risk assessments. This development signifies a substantial advancement in the proactive management of traffic safety. Hussain et al. (2024) explored the enhancement of pedestrian crash estimation using machine learning techniques focused on anomaly detection. Their study addresses the limitations of traditional Extreme Value Theory (EVT) models by applying unconventional sampling methods, thereby increasing the accuracy and reducing uncertainty in crash risk estimations. The use of YOLO for object detection and DeepSORT for tracking is pivotal in this methodology, enhancing detection accuracy and tracking reliability in real-time scenarios. Ghaziamin et al. (2024) developed a privacy-preserving real-time passenger counting system for bus stops using overhead fisheye cameras. This innovative system employs YOLOv4, Detecnet-V2 and Faster-RCNN for detection purposes and DeepSORT for tracking. The system processes data in real-time at 30 frames per second (FPS) when utilizing YOLOv4 as the detection model. This technology significantly enhances transit planning by providing accurate passenger counts, while also maintaining passenger privacy and energy efficiency.

Additionally, Pedestrian-vehicle conflict prediction was explored by Zhang et al. (2020) proposed a model employing a Long Short-Term Memory (LSTM) neural network to forecast pedestrian-vehicle conflicts at signalized intersections by analyzing video data. The model uses YOLOv3 for object detection and Deep SORT for tracking, achieving impressive accuracy rates and demonstrating the transformative potential of LSTM networks in collision warning systems. This approach suggests a proactive enhancement of pedestrian safety in connected vehicle environments.

Crossing intention prediction and behavioral analysis was explored by Zhang et al. (2020) utilized an LSTM neural network to predict pedestrian red-light crossing intentions at intersections by analyzing video data of real traffic scenarios. The model uses YOLOv3 for detection and DeepSORT for tracking, recognizing patterns that indicate potential red-light crossings with a high accuracy rate. This capability aims to improve traffic safety through vehicle-to-infrastructure communication systems that alert drivers to potential pedestrian violations, thereby preventing accidents. Yang et al. (2022) introduced the VENUS smart node, a cooperative traffic signal assistance system for non-motorized users and individuals with disabilities. This novel infrastructure leverages computer vision and edge AI to integrate real-time data on pedestrian movement and intent. The system employs YOLOv4 for detection and OpenPose for pose estimation, achieving high accuracy in detecting crossing intentions and mobility status across various test sites. This innovation has significant potential for widespread use in smart city infrastructures, greatly enhancing safety and accessibility.

Jiao and Fei (2023) conducted a study on monitoring pedestrian walking speeds at the street level using drones. The research utilized UAV-based video footage to measure the walking speeds of pedestrians on a commercial street. Deep learning algorithms, particularly YOLOv5 for object detection and DeepSORT for tracking, were employed in this study. Speed calculations were adjusted for geometric distortions using the SIFT and RANSAC algorithms, achieving high accuracy. The study found that 90.5% of the corrected speeds had an absolute error of less than 0.1 m/s, providing a precise and non-intrusive method for analyzing pedestrian walking speeds. Wang et al. (2024) used drone-captured video footage to examine “safe spaces” for pedestrians and e-bicyclists at urban crosswalks. The study discovered that e-bicyclists maintain larger semi-elliptical safe zones that are sensitive to speed changes compared to the semi-circular zones maintained by pedestrians. By quantifying these safe spaces and examining variations due to speed and traffic presence, the study offers valuable insights for enhancing crosswalk safety and managing urban traffic more effectively. The use of YOLOv3 for object detection and DeepSORT for tracking plays a critical role in this analysis. Zhou et al. (2023) developed an innovative model that integrates a pedestrian-centric environment graph with Graph Convolutional Networks (GCNs) and a pedestrian-state encoder. This model effectively captures dynamic interactions between pedestrians and their environments, providing advanced safety warnings by predicting crossing intentions up to three seconds in advance. This model holds significant potential for applications in intelligent transportation systems. The integration of YOLOv5 for detection, DeepSORT for tracking, and HRNet for pose estimation enhances the model’s predictive accuracy and real-time application. Table 3 illustrates different applications of YOLO usage in pedestrian pose estimation, for intention prediction and behavioral analysis.

Table 3 Studies on YOLO usage in pedestrian pose estimation, for intention prediction and behavioral analysis
Full size table
5.1.2 Traffic sign detection
Traffic sign detection and recognition systems play a pivotal role in enhancing road safety and are essential for the advancement of autonomous driving. These systems enable drivers, or autonomous vehicles, to effectively respond to road conditions, ensuring the safety of all road users (Flores-Calero et al. 2024). However, the complexity and variability of traffic environments, such as adverse weather conditions, combined with the small size of traffic signs present significant challenges for accurately detecting small traffic signs in real-world scenarios (Li et al. 2023; Mahaur and Mishra 2023; Zhang et al. 2020)

For instance, Li et al. (2022) presented a classical YOLO-based architecture for traffic sign recognition. First, traffic signs are categorized and preprocessed according to their specific characteristics. The processed images are then input into an optimized convolutional neural network for finer category classification. The proposed recognition algorithm was tested using a dataset based on the German traffic sign recognition standard, and its performance was compared with other baseline algorithms. Results show that the algorithm significantly improves processing speed while maintaining high classification accuracy, making it better suited for traffic sign recognition systems.

Zhang et al. (2017) presented a Chinese traffic sign detection algorithm based on a deep convolutional network. To enable real-time detection, they proposed an end-to-end convolutional network inspired by YOLOv2. Considering the characteristics of traffic signs, they incorporated multiple 1
1 convolutional layers in the intermediate network layers while reducing the number of convolutional layers in the top layers to decrease computational complexity. For effective small traffic sign detection, the input images are divided into dense grids to capture finer feature maps. Additionally, they expanded the Chinese Traffic Sign Dataset (CTSD) and enhanced the marker information available online. Experimental results using both the expanded CTSD and the German Traffic Sign Detection Benchmark (GTSDB) demonstrate that the proposed method is faster and more robust.

On the other hand, Zhang et al. (2020) proposed a new detection scheme, MSA_YOLOv3, for accurate real-time localization and classification of small traffic signs. The approach begins with data augmentation using image mixup technology. A multi-scale spatial pyramid pooling block is incorporated into the Darknet53 network, enabling more comprehensive learning of object features. Additionally, a bottom-up augmented path is designed to enhance the feature pyramid in YOLOv3, allowing effective utilization of fine-grained features in the lower layers for precise object localization. Tests on the TT100K dataset show that MSA_YOLOv3 outperforms YOLOv3 in detecting small traffic signs.

Recently, later versions of YOLO are being applied to detect traffic signs. For example, Mahaur and Mishra (2023) presented a new version called iS-YOLOv5 model, which increases the mean Average Precision (mAP) by 3.35% on the BDD100K dataset to detect traffic sign and traffic lights. While, Bai et al. (2023) introduced two innovative traffic sign detection models, called YOLOv5-DH and YOLOv5-TDHSA, based on the YOLOv5s model with the following modifications (YOLOv5-DH uses only the second modification): (1) replacing the last layer of the ‘Conv + Batch Normalization + SiLU’ (CBS) structure in the YOLOv5s backbone with a transformer self-attention module (T in the YOLOv5-TDHSA’s name), and also adding a similar module to the last layer of its neck, so that the image information can be used more comprehensively, (2) replacing the YOLOv5s coupled head with a decoupled head (DH in both models’ names) to increase the detection accuracy and speed up the convergence, and (3) adding a small-object detection layer (S in the YOLOv5-TDHSA’s name) and an adaptive anchor (A in the YOLOv5-TDHSA’s name) to the YOLOv5s neck to improve the detection of small objects. Their experiments were conducted using the TT100K dataset.

Similarly, Li et al. (2023) proposed a small object detection algorithm for traffic signs based on the improved YOLOv7 called SANO-YOLOv7. First, the small target detection layer in the neck region was added to augment the detection capability for small traffic sign targets. Simultaneously, the integration of self-attention and convolutional mix modules (ACmix) was applied to the newly added small target detection layer, enabling the capture of additional feature information through the convolutional and self-attention channels within ACmix. Furthermore, the feature extraction capability of the convolution modules was enhanced by replacing the regular convolution modules in the neck layer with omni-dimensional dynamic convolution (ODConv). To further enhance the accuracy of small target detection, the normalized Gaussian Wasserstein distance (NWD) metric was introduced to mitigate the sensitivity to minor positional deviations of small objects. SANO-YOLOv7 was trained on the highly challenging TT100K public dataset.

More interesting traffic sign detection studies were presented with YOLOv8 and later YOLO versions. In the Robotaxi-Full Scale Autonomous Vehicle Competition, YOLOv8 was specifically adapted to recognize and interpret traffic signs, providing real-time alerts that are essential for safe driving (Soylu and Soylu 2024). Next, Zhang (2024) introduced an enhanced traffic sign detection algorithm based on YOLOv9. AKConv replaces the Conv module in RepNCSPELAN4, maintaining detection accuracy while reducing weight. Focal-EIoU Loss replaces the original regression loss function, Clou Loss, accelerating convergence and improving accuracy by dividing the aspect ratio’s loss term into the difference between the minimum outer frame’s width and height and the predicted width and height. Additionally, the network’s feature extraction capability and detection accuracy are further strengthened by incorporating the Convolutional Block Attention Module (CBAM) attention mechanism. The public TT100K traffic sign dataset was used for training and evaluation.

Up until now, YOLO versions 10, 11 and 12 have not been applied in detecting traffic signs. This presents an exciting opportunity for future research and application, as these versions may offer improvements in accuracy, speed, and adaptability for detecting traffic signs in complex environments (4).

Table 4 Studies on YOLO usage in traffic sig

9 YOLO and environmental impact
Training and retraining YOLO is extremely energy-intensive, leading to substantial energy and water consumption, as well as significant carbon dioxide emissions. This environmental impact underscores concerns about the sustainability of AI development, emphasizing the urgent need for more efficient practices to reduce the ecological footprint of large-scale model training (Xu et al. 2024; Dhar 2020).

10 Conclusion
In this comprehensive review, we explored the evolution of the YOLO models from the most recent YOLOv12 to the inaugural YOLOv1, including alternative versions of YOLO as YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and Gold-YOLO. This retrospective analysis covered a decade of advancements, highlighting theapplied use of each version and their respective impacts across five critical application areas: autonomous vehicles and traffic safety, healthcare and medical imaging, security and surveillance, manufacturing, and agriculture. Our review outlined the significant enhancements in detection speed, accuracy, and computational efficiency that each iteration brought, while also addressing the specific challenges and limitations faced by earlier versions. Furthermore, we identified gaps in the current capabilities of YOLO models and proposed potential directions for future research, such as trade-off between detection speed versus accuracy, handling small and overlapping Objects, and generalization across diverse datasets and domains. Predicting the trajectory of YOLO’s development, we anticipate a shift towards multimodal data processing, leveraging advancements in large language models and natural language processing to enhance object detection systems. This fusion is expected to broaden the utility of YOLO models, enabling more sophisticated, context-aware applications that could revolutionize the interaction between AI systems and their environments using Generative AI and multi-modal LLMs. Thus, this review not only serves as a detailed chronicle of YOLO’s evolution but also sets a prospective blueprint for its integration into the next generation of technological innovations.

    